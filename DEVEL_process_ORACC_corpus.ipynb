{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "\n",
    "import joblib\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "ROOT_PATH = os.getcwd()\n",
    "\n",
    "PROJECTS_DATA_PATH = os.path.join(ROOT_PATH, 'projectsdata')\n",
    "CORPUS_PATH = os.path.join(ROOT_PATH, 'CORPUS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and downloading corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add the download functions here (from download_ORACC-JSON.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Listing projects (for Flask app) \"\"\"\n",
    "projects_dict = {}\n",
    "\n",
    "texts_in_project = {}\n",
    "\n",
    "def find_corpusjson_folders_projects(start_path):\n",
    "    data = {}\n",
    "\n",
    "    key = 0\n",
    "    \n",
    "    for root, dirs, files in os.walk(start_path):\n",
    "        if \"corpusjson\" in dirs:\n",
    "            relative_path = os.path.relpath(os.path.join(root, \"corpusjson\"), start_path)\n",
    "\n",
    "            relative_path_metadata = os.path.relpath(os.path.join(root, \"metadata.json\"), start_path)\n",
    "\n",
    "            data[key] = {\n",
    "                \"corpusjson_paths\": relative_path.replace(\"\\\\\", \"/\"),\n",
    "                \"metadata_path\": relative_path_metadata.replace(\"\\\\\", \"/\")\n",
    "            }\n",
    "            \n",
    "\n",
    "            dirs.remove(\"corpusjson\")  # Prevent recursion to the corpusjson folder\n",
    "            key += 1\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def list_project_texts(project_name:str):\n",
    "    texts_with_errors = []\n",
    "    \n",
    "    # Find corpusjson folders in the project:\n",
    "    data = find_corpusjson_folders_projects(os.path.join(PROJECTS_DATA_PATH, project_name))\n",
    "                \n",
    "    project_data = {}\n",
    "    \n",
    "    for key in data:\n",
    "        full_path = os.path.join(PROJECTS_DATA_PATH, project_name, data[key][\"corpusjson_paths\"])\n",
    "        metadata_path = os.path.join(PROJECTS_DATA_PATH, project_name, data[key][\"metadata_path\"])\n",
    "\n",
    "        files_in_folder = os.listdir(full_path)\n",
    "        text_ids = []\n",
    "\n",
    "        if len(files_in_folder) > 0:\n",
    "            print(f'Found {len(files_in_folder)} files in {project_name}/{full_path[:-11]} project')\n",
    "            \n",
    "            for json_file_name in files_in_folder:\n",
    "                text_ids.append(json_file_name[:-5])  # Remove .json extension\n",
    "\n",
    "        with open(metadata_path, 'r', encoding='utf-8') as f:\n",
    "            metadata = json.load(f)\n",
    "            full_project_name = metadata['config']['name']\n",
    "\n",
    "        pd_key = f'{project_name}/{data[key][\"corpusjson_paths\"][:-11]}'\n",
    "        if pd_key.endswith('/'):\n",
    "            pd_key = pd_key[:-1]\n",
    "\n",
    "        project_data[pd_key] = {'full_project_name': full_project_name, 'text_id': text_ids}\n",
    "\n",
    "    return project_data\n",
    "\n",
    "\n",
    "def extract_info(filename='all_projects_jsons'):\n",
    "    all_project_data = {}\n",
    "\n",
    "    for project_name in os.listdir(PROJECTS_DATA_PATH):\n",
    "        project_data = list_project_texts(project_name)\n",
    "\n",
    "        for key, value in project_data.items():\n",
    "            all_project_data[key] = value\n",
    "\n",
    "    return all_project_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 89 files in adsd/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\adsd\\adart1 project\n",
      "Found 162 files in adsd/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\adsd\\adart2 project\n",
      "Found 156 files in adsd/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\adsd\\adart3 project\n",
      "Found 105 files in adsd/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\adsd\\adart5 project\n",
      "Found 180 files in adsd/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\adsd\\adart6 project\n",
      "Found 2 files in aemw/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\aemw\\alalakh/idrimi project\n",
      "Found 305 files in aemw/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\aemw\\amarna project\n",
      "Found 32 files in akklove/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\akklove project\n",
      "Found 175 files in ario/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\ario project\n",
      "Found 160 files in asbp/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\asbp\\ninmed project\n",
      "Found 27 files in asbp/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\asbp\\rlasb project\n",
      "Found 44 files in atae/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\atae\\burmarina project\n",
      "Found 408 files in atae/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\atae\\durkatlimmu project\n",
      "Found 212 files in atae/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\atae\\guzana project\n",
      "Found 19 files in atae/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\atae\\huzirina project\n",
      "Found 30 files in atae/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\atae\\imgurenlil project\n",
      "Found 74 files in atae/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\atae\\mallanate project\n",
      "Found 46 files in atae/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\atae\\marqasu project\n",
      "Found 2 files in atae/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\atae\\samal project\n",
      "Found 22 files in atae/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\atae\\szibaniba project\n",
      "Found 22 files in atae/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\atae\\tilbarsip project\n",
      "Found 33 files in atae/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\atae\\tuszhan project\n",
      "Found 225 files in babcity/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\babcity project\n",
      "Found 2990 files in balt/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\balt project\n",
      "Found 229 files in blms/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\blms project\n",
      "Found 224 files in borsippa/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\borsippa project\n",
      "Found 132 files in btto/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\btto project\n",
      "Found 3 files in cams/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\cams\\barutu project\n",
      "Found 585 files in cams/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\cams\\gkab project\n",
      "Found 59 files in cams/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\cams\\ludlul project\n",
      "Found 3 files in cams/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\cams\\selbi project\n",
      "Found 10 files in cams/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\cams\\tlab project\n",
      "Found 92 files in ckst/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\ckst project\n",
      "Found 49 files in cmawro/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\cmawro\\cmawr1 project\n",
      "Found 82 files in cmawro/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\cmawro\\cmawr2 project\n",
      "Found 124 files in cmawro/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\cmawro\\cmawr3 project\n",
      "Found 9 files in cmawro/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\cmawro\\maqlu project\n",
      "Found 378 files in contrib/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\contrib\\amarna project\n",
      "Found 4978 files in dcclt/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\dcclt project\n",
      "Found 156 files in dcclt/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\dcclt\\ebla project\n",
      "Found 671 files in dcclt/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\dcclt\\nineveh project\n",
      "Found 160 files in dcclt/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\dcclt\\signlists project\n",
      "Found 252 files in dccmt/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\dccmt project\n",
      "Found 535 files in dsst/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\dsst project\n",
      "Found 816 files in ecut/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\ecut project\n",
      "Found 976 files in edlex/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\edlex project\n",
      "Found 523 files in eisl/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\eisl project\n",
      "Found 1456 files in etcsri/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\etcsri project\n",
      "Found 20 files in glass/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\glass project\n",
      "Found 487 files in hbtin/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\hbtin project\n",
      "Found 9 files in iraq/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\iraq\\iraq85 project\n",
      "Found 362 files in kish/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\kish project\n",
      "Found 1 files in nere/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\nere project\n",
      "Found 550 files in obel/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\obel project\n",
      "Found 201 files in obmc/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\obmc project\n",
      "Found 35 files in obta/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\obta project\n",
      "Found 40 files in ribo/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\ribo\\bab7scores project\n",
      "Found 1 files in ribo/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\ribo\\babylon10 project\n",
      "Found 38 files in ribo/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\ribo\\babylon2 project\n",
      "Found 4 files in ribo/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\ribo\\babylon3 project\n",
      "Found 6 files in ribo/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\ribo\\babylon4 project\n",
      "Found 1 files in ribo/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\ribo\\babylon5 project\n",
      "Found 126 files in ribo/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\ribo\\babylon6 project\n",
      "Found 232 files in ribo/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\ribo\\babylon7 project\n",
      "Found 3 files in ribo/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\ribo\\babylon8 project\n",
      "Found 400 files in ribo/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\ribo\\sources project\n",
      "Found 378 files in rimanum/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\rimanum project\n",
      "Found 96 files in rinap/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\rinap\\rinap1 project\n",
      "Found 151 files in rinap/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\rinap\\rinap2 project\n",
      "Found 261 files in rinap/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\rinap\\rinap3 project\n",
      "Found 183 files in rinap/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\rinap\\rinap4 project\n",
      "Found 346 files in rinap/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\rinap\\rinap5 project\n",
      "Found 129 files in rinap/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\rinap\\scores project\n",
      "Found 2209 files in rinap/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\rinap\\sources project\n",
      "Found 265 files in saao/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\saao\\saa01 project\n",
      "Found 15 files in saao/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\saao\\saa02 project\n",
      "Found 52 files in saao/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\saao\\saa03 project\n",
      "Found 353 files in saao/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\saao\\saa04 project\n",
      "Found 300 files in saao/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\saao\\saa05 project\n",
      "Found 350 files in saao/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\saao\\saa06 project\n",
      "Found 219 files in saao/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\saao\\saa07 project\n",
      "Found 568 files in saao/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\saao\\saa08 project\n",
      "Found 11 files in saao/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\saao\\saa09 project\n",
      "Found 389 files in saao/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\saao\\saa10 project\n",
      "Found 234 files in saao/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\saao\\saa11 project\n",
      "Found 98 files in saao/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\saao\\saa12 project\n",
      "Found 210 files in saao/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\saao\\saa13 project\n",
      "Found 479 files in saao/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\saao\\saa14 project\n",
      "Found 390 files in saao/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\saao\\saa15 project\n",
      "Found 246 files in saao/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\saao\\saa16 project\n",
      "Found 207 files in saao/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\saao\\saa17 project\n",
      "Found 205 files in saao/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\saao\\saa18 project\n",
      "Found 229 files in saao/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\saao\\saa19 project\n",
      "Found 55 files in saao/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\saao\\saa20 project\n",
      "Found 161 files in saao/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\saao\\saa21 project\n",
      "Found 23 files in saao/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\saao\\saas2 project\n",
      "Found 33 files in suhu/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\suhu project\n",
      "Found 147 files in urap/c:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\projectsdata\\urap project\n"
     ]
    }
   ],
   "source": [
    "data_info = extract_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project: adsd\n",
      "Project: adsd/adart1\n",
      "Project: adsd/adart2\n",
      "Project: adsd/adart3\n",
      "Project: adsd/adart5\n",
      "Project: adsd/adart6\n",
      "Project: aemw/alalakh/idrimi\n",
      "Project: aemw/amarna\n",
      "Project: akklove\n",
      "Project: amgg\n",
      "Project: ario\n",
      "Project: armep\n",
      "Project: arrim\n",
      "Project: asbp\n",
      "Project: asbp/ninmed\n",
      "Project: asbp/rlasb\n",
      "Project: atae\n",
      "Project: atae/assur\n",
      "Project: atae/burmarina\n",
      "Project: atae/durkatlimmu\n",
      "Project: atae/durszarrukin\n",
      "Project: atae/guzana\n",
      "Project: atae/huzirina\n",
      "Project: atae/imgurenlil\n",
      "Project: atae/kalhu\n",
      "Project: atae/kunalia\n",
      "Project: atae/mallanate\n",
      "Project: atae/marqasu\n",
      "Project: atae/nineveh\n",
      "Project: atae/samal\n",
      "Project: atae/szibaniba\n",
      "Project: atae/tilbarsip\n",
      "Project: atae/tuszhan\n",
      "Project: babcity\n",
      "Project: balt\n",
      "Project: blms\n",
      "Project: borsippa\n",
      "Project: btmao\n",
      "Project: btto\n",
      "Project: cams/barutu\n",
      "Project: cams/gkab\n",
      "Project: cams/ludlul\n",
      "Project: cams/selbi\n",
      "Project: cams/tlab\n",
      "Project: ckst\n",
      "Project: cmawro\n",
      "Project: cmawro/cmawr1\n",
      "Project: cmawro/cmawr2\n",
      "Project: cmawro/cmawr3\n",
      "Project: cmawro/maqlu\n",
      "Project: contrib/amarna\n",
      "Project: contrib/jacobsen\n",
      "Project: contrib/lambert\n",
      "Project: dcclt\n",
      "Project: dcclt/ebla\n",
      "Project: dcclt/jena\n",
      "Project: dcclt/nineveh\n",
      "Project: dcclt/signlists\n",
      "Project: dccmt\n",
      "Project: dsst\n",
      "Project: ecut\n",
      "Project: edlex\n",
      "Project: eisl\n",
      "Project: etcsri\n",
      "Project: glass\n",
      "Project: hbtin\n",
      "Project: iraq\n",
      "Project: iraq/iraq85\n",
      "Project: kish\n",
      "Project: kish/fieldmus\n",
      "Project: kish/fieldmus/fmod\n",
      "Project: kish/mathaffield\n",
      "Project: nere\n",
      "Project: nimrud\n",
      "Project: obel\n",
      "Project: obmc\n",
      "Project: obta\n",
      "Project: ogsl\n",
      "Project: oimea\n",
      "Project: osl\n",
      "Project: pnao\n",
      "Project: qcat\n",
      "Project: riao\n",
      "Project: ribo\n",
      "Project: ribo/bab7scores\n",
      "Project: ribo/babylon10\n",
      "Project: ribo/babylon2\n",
      "Project: ribo/babylon3\n",
      "Project: ribo/babylon4\n",
      "Project: ribo/babylon5\n",
      "Project: ribo/babylon6\n",
      "Project: ribo/babylon7\n",
      "Project: ribo/babylon8\n",
      "Project: ribo/sources\n",
      "Project: rimanum\n",
      "Project: rinap\n",
      "Project: rinap/rinap1\n",
      "Project: rinap/rinap2\n",
      "Project: rinap/rinap3\n",
      "Project: rinap/rinap4\n",
      "Project: rinap/rinap5\n",
      "Project: rinap/scores\n",
      "Project: rinap/sources\n",
      "Project: saao\n",
      "Project: saao/aebp\n",
      "Project: saao/knpp\n",
      "Project: saao/saa01\n",
      "Project: saao/saa02\n",
      "Project: saao/saa03\n",
      "Project: saao/saa04\n",
      "Project: saao/saa05\n",
      "Project: saao/saa06\n",
      "Project: saao/saa07\n",
      "Project: saao/saa08\n",
      "Project: saao/saa09\n",
      "Project: saao/saa10\n",
      "Project: saao/saa11\n",
      "Project: saao/saa12\n",
      "Project: saao/saa13\n",
      "Project: saao/saa14\n",
      "Project: saao/saa15\n",
      "Project: saao/saa16\n",
      "Project: saao/saa17\n",
      "Project: saao/saa18\n",
      "Project: saao/saa19\n",
      "Project: saao/saa20\n",
      "Project: saao/saa21\n",
      "Project: saao/saas2\n",
      "Project: suhu\n",
      "Project: tcma\n",
      "Project: tsae\n",
      "Project: urap\n",
      "Project: xcat\n",
      "Number of projects (full): 133\n",
      "Number of projects (with texts): 98\n",
      "Number of texts: 29530\n",
      "Number of subprojects: 86\n",
      "Number of projects (HEAD): 47\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Listing number of projects and number of texts. \"\"\"\n",
    "\n",
    "number_of_subprojects = 0\n",
    "number_of_projects_HEAD = 0\n",
    "number_of_projects_full = 0\n",
    "number_of_projects_with_texts = 0\n",
    "number_of_texts = 0\n",
    "\n",
    "for project, info in data_info.items():\n",
    "    number_of_projects_full += 1\n",
    "    texts_in_project = 0\n",
    "\n",
    "    print(\"Project:\", project)\n",
    "\n",
    "    for text_id in info['text_id']:\n",
    "        number_of_texts += 1\n",
    "        texts_in_project += 1\n",
    "\n",
    "    if texts_in_project > 0:\n",
    "        number_of_projects_with_texts += 1\n",
    "\n",
    "    if '/' not in project:\n",
    "        number_of_projects_HEAD += 1\n",
    "    else:\n",
    "        number_of_subprojects += 1\n",
    "\n",
    "print(\"Number of projects (full):\", number_of_projects_full)\n",
    "print(\"Number of projects (with texts):\", number_of_projects_with_texts)\n",
    "print(\"Number of texts:\", number_of_texts)\n",
    "print(\"Number of subprojects:\", number_of_subprojects)\n",
    "print(\"Number of projects (HEAD):\", number_of_projects_HEAD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{\\n\"projects\": [')\n",
    "\n",
    "for project, info in data_info.items():\n",
    "    print('\\t{ \"value\": \"'+project+'\", \"label\": \"'+info['full_project_name']+'\" },')\n",
    "\n",
    "print('\\t],\\n\"textsByProject\": {')\n",
    "for project, info in data_info.items():\n",
    "    print(f'\\t\"{project}\": [')\n",
    "    i=0\n",
    "    for text_id in info['text_id']:\n",
    "        if i == len(info['text_id']) - 1:\n",
    "            print(f'\\t\\t{{\"value\": \"{project}/{text_id}\"}}')\n",
    "        else:\n",
    "            print(f'\\t\\t{{\"value\": \"{project}/{text_id}\"}},')\n",
    "        i += 1\n",
    "    print('\\t],')\n",
    "\n",
    "print('\\t}\\n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_corpusjson_folders(start_path):\n",
    "    corpusjson_paths = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(start_path):\n",
    "        if \"corpusjson\" in dirs:\n",
    "            relative_path = os.path.relpath(os.path.join(root, \"corpusjson\"), start_path)\n",
    "            corpusjson_paths.append(relative_path.replace(\"\\\\\", \"/\"))\n",
    "            dirs.remove(\"corpusjson\")  # Prevent recursion to the corpusjson folder\n",
    "    \n",
    "    return corpusjson_paths\n",
    "\n",
    "\n",
    "def extract_jsons_from_project(project_name:str):\n",
    "    texts_with_errors = []\n",
    "    \n",
    "    # Find corpusjson folders in the project:\n",
    "    corpusjson_folders = find_corpusjson_folders(os.path.join(PROJECTS_DATA_PATH, project_name))\n",
    "                \n",
    "    project_jsons = {}\n",
    "    \n",
    "    for corpusjson_folder in corpusjson_folders:\n",
    "        full_path = os.path.join(PROJECTS_DATA_PATH, project_name, corpusjson_folder)\n",
    "        text_id_prefix = f'{project_name}/{corpusjson_folder[:-11]}'\n",
    "        files_in_folder = os.listdir(full_path)\n",
    "        if len(files_in_folder) > 0:\n",
    "            print(f'Found {len(files_in_folder)} files in {project_name}/{corpusjson_folder[:-11]} project')\n",
    "            \n",
    "            for json_file_name in files_in_folder:\n",
    "                with open(os.path.join(full_path, json_file_name), 'r', encoding='utf-8') as json_file:\n",
    "                    text_id = f'{text_id_prefix}/{json_file_name[:-5]}'.replace('//', '/') # in case there are no subprojects, there are double slashes --> remove them\n",
    "                    #print(text_id)\n",
    "                    try:\n",
    "                        json_data = json.load(json_file)\n",
    "                        project_jsons[text_id] = json_data\n",
    "                    except:\n",
    "                        texts_with_errors.append(text_id)\n",
    "                        \n",
    "    return project_jsons, texts_with_errors\n",
    "\n",
    "def save_json_corpus(json_corpus:dict, save_name:str, save_path=CORPUS_PATH, compression=None):\n",
    "    \"\"\" Save the ORACC corpus to a joblib file. \"\"\"\n",
    "    if compression:\n",
    "        joblib.dump(json_corpus, os.path.join(save_path, f'{save_name}.joblib'), compress=compression)\n",
    "    else:\n",
    "        joblib.dump(json_corpus, os.path.join(save_path, f'{save_name}.joblib'))\n",
    "\n",
    "def save_all_projects_jsons(filename='all_projects_jsons'):\n",
    "    all_project_jsons = {}\n",
    "    projects_texts_with_errors = {}\n",
    "\n",
    "    for project_name in os.listdir(PROJECTS_DATA_PATH):\n",
    "        project_jsons, texts_with_errors = extract_jsons_from_project(project_name)\n",
    "        all_project_jsons[project_name] = project_jsons\n",
    "        projects_texts_with_errors[project_name] = texts_with_errors\n",
    "\n",
    "    save_json_corpus(all_project_jsons, filename)\n",
    "    save_json_corpus(projects_texts_with_errors, f'{filename}_texts_with_errors')\n",
    "\n",
    "\"\"\" Variant: try to create the corpusjson file that is smaller (ignoring many data). \"\"\"\n",
    "DROP_KEYS = {\n",
    "    \"type\",\"implicit\",\"id\",\"ref\",\"lang\",\"role\",\"gg\",\"gdl_type\",\"name\",\n",
    "    \"ngram\",\"delim\",\"inst\",\"break\",\"ho\",\"hc\",\"value\",\"subtype\",\"label\",\"sig\"\n",
    "}\n",
    "\n",
    "def prune_obj(x, drop=DROP_KEYS):\n",
    "    \"\"\"Rekurzivně odstraní z dictů zadané klíče, projde i seznamy.\"\"\"\n",
    "    if isinstance(x, dict):\n",
    "        return {k: prune_obj(v, drop) for k, v in x.items() if k not in drop}\n",
    "    if isinstance(x, list):\n",
    "        return [prune_obj(v, drop) for v in x]\n",
    "    return x\n",
    "\n",
    "\n",
    "def save_individual_project_jsons(prefix:str='project_', prune=True, compression=None):\n",
    "    \"\"\" Saving individual joblib files for each project - possibly better for RAM. \"\"\"\n",
    "\n",
    "    projects_texts_with_errors = {}\n",
    "\n",
    "    for project_name in os.listdir(PROJECTS_DATA_PATH):\n",
    "        project_jsons, texts_with_errors = extract_jsons_from_project(project_name)\n",
    "        projects_texts_with_errors[project_name] = texts_with_errors\n",
    "\n",
    "        if prune:\n",
    "            project_jsons = prune_obj(project_jsons)\n",
    "\n",
    "        save_json_corpus(project_jsons, f'{prefix}{project_name}', compression=compression)\n",
    "\n",
    "    save_json_corpus(projects_texts_with_errors, f'{prefix}texts_with_errors_for_individual_files', compression=compression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 89 files in adsd/adart1 project\n",
      "Found 162 files in adsd/adart2 project\n",
      "Found 156 files in adsd/adart3 project\n",
      "Found 105 files in adsd/adart5 project\n",
      "Found 180 files in adsd/adart6 project\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "LZ4 is not installed. Install it with pip: https://python-lz4.readthedocs.io/",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43msave_individual_project_jsons\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlz4_pruned\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlz4\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Save individual projects to JSON files\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# save_all_projects_jsons(mode='json')  # Save all projects to JSON files\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 86\u001b[39m, in \u001b[36msave_individual_project_jsons\u001b[39m\u001b[34m(prefix, prune, compression)\u001b[39m\n\u001b[32m     83\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m prune:\n\u001b[32m     84\u001b[39m         project_jsons = prune_obj(project_jsons)\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     \u001b[43msave_json_corpus\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproject_jsons\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mproject_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m save_json_corpus(projects_texts_with_errors, \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mtexts_with_errors_for_individual_files\u001b[39m\u001b[33m'\u001b[39m, compression=compression)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36msave_json_corpus\u001b[39m\u001b[34m(json_corpus, save_name, save_path, compression)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\" Save the ORACC corpus to a joblib file. \"\"\"\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     \u001b[43mjoblib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_corpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msave_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.joblib\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompress\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     45\u001b[39m     joblib.dump(json_corpus, os.path.join(save_path, \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.joblib\u001b[39m\u001b[33m'\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\valek\\PthonProjectsOutsideOD\\ORACCIntertext\\.venv\\Lib\\site-packages\\joblib\\numpy_pickle.py:547\u001b[39m, in \u001b[36mdump\u001b[39m\u001b[34m(value, filename, compress, protocol)\u001b[39m\n\u001b[32m    544\u001b[39m     compress_level = compress\n\u001b[32m    546\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m compress_method == \u001b[33m\"\u001b[39m\u001b[33mlz4\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m lz4 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m547\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(LZ4_NOT_INSTALLED_ERROR)\n\u001b[32m    549\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    550\u001b[39m     compress_level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    551\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m compress_level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    552\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m compress_level \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m10\u001b[39m)\n\u001b[32m    553\u001b[39m ):\n\u001b[32m    554\u001b[39m     \u001b[38;5;66;03m# Raising an error if a non valid compress level is given.\u001b[39;00m\n\u001b[32m    555\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    556\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mNon valid compress level given: \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m. Possible values are \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m.format(\n\u001b[32m    557\u001b[39m             compress_level, \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[32m10\u001b[39m))\n\u001b[32m    558\u001b[39m         )\n\u001b[32m    559\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: LZ4 is not installed. Install it with pip: https://python-lz4.readthedocs.io/"
     ]
    }
   ],
   "source": [
    "save_individual_project_jsons(prefix='lz4_pruned', compression=('lz4', 3))  # Save individual projects to JSON files\n",
    "\n",
    "# save_all_projects_jsons(mode='json')  # Save all projects to JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NOTE: in case of POS variants, we want to ignore named entities/Proper Nouns. (see https://oracc.museum.upenn.edu/doc/help/languages/propernouns/index.html)\n",
    "# PN_POSs = ['AN', 'CN', 'DN', 'EN', 'FN', 'GN', 'LN', 'MN', 'ON', 'PN', 'QN', 'RN', 'SN', 'TN', 'WN', 'YN']\n",
    "\n",
    "# def load_json_corpus(json_corpus_name:str, load_path=CORPUS_PATH) -> dict:\n",
    "#     return joblib.load(os.path.join(load_path, f'{json_corpus_name}.joblib'))\n",
    "\n",
    "# def parsejson(text_json:dict):\n",
    "#     text_forms = []\n",
    "#     text_lemma = []\n",
    "#     text_normalised = []\n",
    "    \n",
    "#     text_signs = []\n",
    "#     text_signs_gdl = []\n",
    "\n",
    "#     text_forms_POS = []\n",
    "#     text_lemma_POS = []\n",
    "#     text_normalised_POS = []\n",
    "\n",
    "#     def extract_from_node(obj):\n",
    "#         if isinstance(obj, dict):\n",
    "#             if obj.get(\"node\") == \"l\" and isinstance(obj.get(\"f\"), dict):\n",
    "#                 f = obj[\"f\"]\n",
    "\n",
    "#                 pos  = f.get(\"pos\") or f.get(\"epos\")\n",
    "#                 form = f.get(\"form\")\n",
    "#                 lemma = f.get(\"cf\")\n",
    "#                 norm = f.get(\"norm\") or f.get(\"norm0\")\n",
    "\n",
    "#                 text_forms.append(form)\n",
    "#                 text_lemma.append(lemma)\n",
    "#                 text_normalised.append(norm)\n",
    "\n",
    "#                 if pos in PN_POSs:\n",
    "#                     text_forms_POS.append(f\"PN_{pos}\")\n",
    "#                     text_lemma_POS.append(f\"PN_{pos}\")\n",
    "#                     text_normalised_POS.append(f\"PN_{pos}\")\n",
    "#                 else:\n",
    "#                     text_forms_POS.append(form)\n",
    "#                     text_lemma_POS.append(lemma)\n",
    "#                     text_normalised_POS.append(norm)\n",
    "\n",
    "#                 for g in f.get(\"gdl\", []):\n",
    "#                     if isinstance(g, dict):\n",
    "#                         if \"v\" in g:\n",
    "#                             text_signs.append(g[\"v\"])\n",
    "#                         if \"gdl_sign\" in g:\n",
    "#                             text_signs_gdl.append(g[\"gdl_sign\"])\n",
    "#                         for sub in g.get(\"seq\", []):\n",
    "#                             if \"v\" in sub:\n",
    "#                                 text_signs.append(sub[\"v\"])\n",
    "#                             if \"gdl_sign\" in sub:\n",
    "#                                 text_signs_gdl.append(sub[\"gdl_sign\"])\n",
    "#             for value in obj.values():\n",
    "#                 extract_from_node(value)\n",
    "#         elif isinstance(obj, list):\n",
    "#             for item in obj:\n",
    "#                 extract_from_node(item)\n",
    "\n",
    "#     def change_unknowns(input_list:list):\n",
    "#         unknowns = [None, 'x', 'X']\n",
    "#         return [\"■\" if item in unknowns else item for item in input_list]\n",
    "\n",
    "#     extract_from_node(text_json)\n",
    "\n",
    "#     text_forms = change_unknowns(text_forms)\n",
    "#     text_lemma = change_unknowns(text_lemma)\n",
    "#     text_normalised = change_unknowns(text_normalised)\n",
    "#     text_signs = change_unknowns(text_signs)\n",
    "#     text_signs_gdl = change_unknowns(text_signs_gdl)\n",
    "#     text_forms_POS = change_unknowns(text_forms_POS)\n",
    "#     text_lemma_POS = change_unknowns(text_lemma_POS)\n",
    "#     text_normalised_POS = change_unknowns(text_normalised_POS)\n",
    "\n",
    "#     return {'text_forms': text_forms, 'text_lemma': text_lemma, 'text_normalised': text_normalised, 'text_signs': text_signs, 'text_signs_gdl': text_signs_gdl, 'text_forms_POS': text_forms_POS, 'text_lemma_POS': text_lemma_POS, 'text_normalised_POS': text_normalised_POS}\n",
    "\n",
    "\n",
    "# def normalize_signs(input_: str) -> str:\n",
    "#     \"\"\" Normalises signs in the text (e.g., ša = ša₂ = ša₃)\"\"\"\n",
    "#     for num in '₁₂₃₄₅₆₇₈₉₀':\n",
    "#         while num in input_:\n",
    "#             input_ = input_.replace(num, '')\n",
    "\n",
    "#     return input_\n",
    "\n",
    "\n",
    "# def normalize_signs_list(input_: list) -> list:\n",
    "#     \"\"\" Normalises signs in the text (e.g., ša = ša₂ = ša₃)\"\"\"\n",
    "#     return [normalize_signs(item) for item in input_]\n",
    "\n",
    "# class OraccProjectCorpus:\n",
    "#     def __init__(self, json_corpus):\n",
    "#         self.corpus = json_corpus\n",
    "#         self.texts =  [text_id for text_id in json_corpus]\n",
    "#         self.texts_data = [json_corpus[text_id] for text_id in json_corpus]\n",
    "#         self.size = len(json_corpus)\n",
    "        \n",
    "#         analysed_corpus, texts_with_errors, empty_texts = self.AnalyseCorpus()\n",
    "        \n",
    "#         self.Lemma = analysed_corpus['lemma']\n",
    "#         self.Forms = analysed_corpus['forms']\n",
    "#         self.Normalised = analysed_corpus['normalised']\n",
    "#         self.Signs = analysed_corpus['signs']\n",
    "#         self.SignsGDL = analysed_corpus['signs_gdl']\n",
    "        \n",
    "#         self.LemmaPOS = analysed_corpus.get('lemma_POS')\n",
    "#         self.FormsPOS = analysed_corpus.get('forms_POS')\n",
    "#         self.NormalisedPOS = analysed_corpus.get('normalised_POS')\n",
    "        \n",
    "#         self.SignsNormalised = analysed_corpus.get('signs_normalised')\n",
    "#         self.FormsNormalised = analysed_corpus.get('forms_normalised')\n",
    "#         self.FormsPOSNormalised = analysed_corpus.get('forms_POS_normalised')\n",
    "\n",
    "#         self.texts_with_errors = texts_with_errors\n",
    "#         self.empty_texts = empty_texts\n",
    "    \n",
    "#     def AnalyseCorpus(self) -> dict: \n",
    "#         texts_with_errors = []\n",
    "#         empty_texts = []\n",
    "        \n",
    "#         corpus_data = {}\n",
    "        \n",
    "#         full_corpus_forms = []\n",
    "#         full_corpus_lemma = []\n",
    "#         full_corpus_normalised = []\n",
    "#         full_corpus_signs = []\n",
    "#         full_corpus_signs_gdl = []\n",
    "#         full_corpus_forms_POS = []\n",
    "#         full_corpus_lemma_POS = []\n",
    "#         full_corpus_normalised_POS = []\n",
    "#         full_corpus_signs_normalised = []\n",
    "#         full_corpus_forms_normalised = []\n",
    "#         full_corpus_forms_POS_normalised = []\n",
    "\n",
    "#         # print('\\tAnalyzing texts in the corpus.', self.size, 'texts to be processed.')\n",
    "        \n",
    "#         for text_id in self.texts:\n",
    "            \n",
    "#             try:\n",
    "#                 text_analysed = parsejson(self.corpus[text_id])\n",
    "#             except:\n",
    "#                 # TODO: find out the problems with these texts!\n",
    "#                 #print('ERROR with a text:', text_id)\n",
    "#                 texts_with_errors.append(text_id)\n",
    "#                 text_analysed = {'text_forms': [], 'text_lemma': [], 'text_normalised': [], 'text_signs': [], 'text_signs_gdl': [], 'text_forms_POS': [], 'text_lemma_POS': [], 'text_normalised_POS': [], 'signs_normalised': [], 'forms_normalised': [], 'forms_POS_normalised': []}\n",
    "\n",
    "#             corpus_data[text_id] = text_analysed\n",
    "            \n",
    "#             full_corpus_forms.append(text_analysed['text_forms'])\n",
    "#             full_corpus_lemma.append(text_analysed['text_lemma'])\n",
    "#             full_corpus_normalised.append(text_analysed['text_normalised'])\n",
    "#             full_corpus_signs.append(text_analysed['text_signs'])\n",
    "#             full_corpus_signs_gdl.append(text_analysed['text_signs_gdl'])\n",
    "#             full_corpus_forms_POS.append(text_analysed['text_forms_POS'])\n",
    "#             full_corpus_lemma_POS.append(text_analysed['text_lemma_POS'])\n",
    "#             full_corpus_normalised_POS.append(text_analysed['text_normalised_POS'])\n",
    "\n",
    "#             full_corpus_signs_normalised.append(normalize_signs_list(text_analysed['text_signs']))\n",
    "#             full_corpus_forms_normalised.append(normalize_signs_list(text_analysed['text_forms']))\n",
    "#             full_corpus_forms_POS_normalised.append(normalize_signs_list(text_analysed['text_forms_POS']))\n",
    "\n",
    "#             if text_analysed == {'text_forms': [], 'text_lemma': [], 'text_normalised': [], 'text_signs': [], 'text_signs_gdl': [], 'text_forms_POS': [], 'text_lemma_POS': [], 'text_normalised_POS': [], 'signs_normalised': [], 'forms_normalised': [], 'forms_POS_normalised': []}:\n",
    "#                 empty_texts.append(text_id)\n",
    "\n",
    "#         return {'corpus_data': corpus_data, 'forms': full_corpus_forms, 'lemma': full_corpus_lemma, 'normalised': full_corpus_normalised, 'signs': full_corpus_signs, 'signs_gdl': full_corpus_signs_gdl, 'forms_POS': full_corpus_forms_POS, 'lemma_POS': full_corpus_lemma_POS, 'normalised_POS': full_corpus_normalised_POS, 'signs_normalised': full_corpus_signs_normalised, 'forms_normalised': full_corpus_forms_normalised, 'forms_POS_normalised': full_corpus_forms_POS_normalised}, texts_with_errors, empty_texts\n",
    "\n",
    "\n",
    "# class OraccCorpus():\n",
    "#     def __init__(self, input_projects:dict) -> None:\n",
    "#         self.projects = input_projects\n",
    "\n",
    "#         all_texts = []\n",
    "        \n",
    "#         lemma_corpus = []\n",
    "#         forms_corpus = []\n",
    "#         normalised_corpus = []\n",
    "#         signs_corpus = []\n",
    "#         signs_gdl_corpus = []\n",
    "#         forms_POS_corpus = []\n",
    "#         lemma_POS_corpus = []\n",
    "#         normalised_POS_corpus = []\n",
    "#         signs_normalised_corpus = []\n",
    "#         forms_normalised_corpus = []\n",
    "#         forms_POS_normalised_corpus = []\n",
    "\n",
    "#         texts_with_errors = []\n",
    "#         empty_texts = []\n",
    "    \n",
    "#         for project_name, project_data in tqdm(input_projects.items(), desc='Processing projects'):\n",
    "#             #print(project_name, 'is being processed for dictionary.')\n",
    "#             OPC_project = OraccProjectCorpus(json_corpus=project_data)\n",
    "#             for text in OPC_project.Lemma:\n",
    "#                 lemma_corpus.append(text)\n",
    "            \n",
    "#             for text in OPC_project.Forms:\n",
    "#                 forms_corpus.append(text)\n",
    "                \n",
    "#             for text in OPC_project.Normalised:\n",
    "#                 normalised_corpus.append(text)\n",
    "\n",
    "#             for text in OPC_project.Signs:\n",
    "#                 signs_corpus.append(text)\n",
    "            \n",
    "#             for text in OPC_project.SignsGDL:\n",
    "#                 signs_gdl_corpus.append(text)\n",
    "\n",
    "#             for text_id in OPC_project.texts_with_errors:\n",
    "#                 texts_with_errors.append(text_id)\n",
    "\n",
    "#             for text_id in OPC_project.empty_texts:\n",
    "#                 empty_texts.append(text_id)\n",
    "\n",
    "#             for text in OPC_project.FormsPOS:\n",
    "#                 forms_POS_corpus.append(text)\n",
    "\n",
    "#             for text in OPC_project.LemmaPOS:\n",
    "#                 lemma_POS_corpus.append(text)\n",
    "\n",
    "#             for text in OPC_project.NormalisedPOS:\n",
    "#                 normalised_POS_corpus.append(text)\n",
    "\n",
    "#             for text_id in OPC_project.texts:\n",
    "#                 all_texts.append(text_id)\n",
    "\n",
    "#             for text in OPC_project.SignsNormalised:\n",
    "#                 signs_normalised_corpus.append(text)\n",
    "\n",
    "#             for text in OPC_project.FormsNormalised:\n",
    "#                 forms_normalised_corpus.append(text)\n",
    "\n",
    "#             for text in OPC_project.FormsPOSNormalised:\n",
    "#                 forms_POS_normalised_corpus.append(text)\n",
    "\n",
    "#         print('Corpus size:', len(lemma_corpus), 'texts.')\n",
    "#         print('Texts with errors:', len(texts_with_errors), 'texts.')\n",
    "#         for text_id in texts_with_errors:\n",
    "#             print('\\t', text_id)\n",
    "\n",
    "#         print('Empty texts:', len(empty_texts), 'texts.')\n",
    "\n",
    "#         self.texts = all_texts\n",
    "#         self.lemma_corpus = lemma_corpus\n",
    "#         self.forms_corpus = forms_corpus\n",
    "#         self.normalised_corpus = normalised_corpus\n",
    "        \n",
    "#         self.signs_corpus = signs_corpus\n",
    "#         self.signs_gdl_corpus = signs_gdl_corpus\n",
    "        \n",
    "#         self.forms_POS_corpus = forms_POS_corpus\n",
    "#         self.lemma_POS_corpus = lemma_POS_corpus\n",
    "#         self.normalised_POS_corpus = normalised_POS_corpus\n",
    "\n",
    "#         self.signs_normalised_corpus = signs_normalised_corpus\n",
    "#         self.forms_normalised_corpus = forms_normalised_corpus\n",
    "#         self.forms_POS_normalised_corpus = forms_POS_normalised_corpus\n",
    "\n",
    "#         self.texts_with_errors = texts_with_errors\n",
    "#         self.empty_texts = empty_texts\n",
    "\n",
    "\n",
    "#     def get_data_by_id(self, text_id, mode='forms', print_=False) -> list:\n",
    "#         \"\"\" Print text data for debugging purposes. \"\"\"\n",
    "#         try:\n",
    "#             txt_idx = self.texts.index(text_id)\n",
    "#         except ValueError:\n",
    "#             print(f'Text ID {text_id} not found in the corpus.')\n",
    "#             return []\n",
    "\n",
    "#         if mode == 'forms':\n",
    "#             if print_:\n",
    "#                 print(f'Forms: {self.forms_corpus[txt_idx]}')\n",
    "#             return self.forms_corpus[txt_idx]\n",
    "#         elif mode == 'lemma':\n",
    "#             if print_:\n",
    "#                 print(f'Lemmas: {self.lemma_corpus[txt_idx]}')\n",
    "#             return self.lemma_corpus[txt_idx]\n",
    "#         elif mode == 'normalised':\n",
    "#             if print_:\n",
    "#                 print(f'Normalised: {self.normalised_corpus[txt_idx]}')\n",
    "#             return self.normalised_corpus[txt_idx]\n",
    "#         elif mode == 'signs':\n",
    "#             if print_:\n",
    "#                 print(f'Signs: {self.signs_corpus[txt_idx]}')\n",
    "#             return self.signs_corpus[txt_idx]\n",
    "#         elif mode == 'signs_gdl':\n",
    "#             if print_:\n",
    "#                 print(f'Signs GDL: {self.signs_gdl_corpus[txt_idx]}')\n",
    "#             return self.signs_gdl_corpus[txt_idx]\n",
    "#         elif mode == 'forms_pos':\n",
    "#             if print_:\n",
    "#                 print(f'Forms POS: {self.forms_POS_corpus[txt_idx]}')\n",
    "#             return self.forms_POS_corpus[txt_idx]\n",
    "#         elif mode == 'lemma_pos':\n",
    "#             if print_:\n",
    "#                 print(f'Lemmas POS: {self.lemma_POS_corpus[txt_idx]}')\n",
    "#             return self.lemma_POS_corpus[txt_idx]\n",
    "#         elif mode == 'normalised_pos':\n",
    "#             if print_:\n",
    "#                 print(f'Normalised POS: {self.normalised_POS_corpus[txt_idx]}')\n",
    "#             return self.normalised_POS_corpus[txt_idx]\n",
    "#         elif mode == 'signs_normalised':\n",
    "#             if print_:\n",
    "#                 print(f'Signs Normalised: {self.signs_normalised_corpus[txt_idx]}')\n",
    "#             return self.signs_normalised_corpus[txt_idx]\n",
    "#         elif mode == 'forms_normalised':\n",
    "#             if print_:\n",
    "#                 print(f'Forms Normalised: {self.forms_normalised_corpus[txt_idx]}')\n",
    "#             return self.forms_normalised_corpus[txt_idx]\n",
    "#         elif mode == 'forms_pos_normalised':\n",
    "#             if print_:\n",
    "#                 print(f'Forms POS Normalised: {self.forms_POS_normalised_corpus[txt_idx]}')\n",
    "#             return self.forms_POS_normalised_corpus[txt_idx]\n",
    "#         else:\n",
    "#             if print_:\n",
    "#                 print(f'Mode print set wrong! Use \"forms\", \"lemma\", \"normalised\", \"signs\", \"signs_gdl\", \"forms_pos\", \"lemma_pos\", \"normalised_pos\", \"signs_normalised\", \"forms_normalised\", or \"forms_pos_normalised\".')\n",
    "#             return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "PN_POSs = ['AN', 'CN', 'DN', 'EN', 'FN', 'GN', 'LN', 'MN', 'ON', 'PN', 'QN', 'RN', 'SN', 'TN', 'WN', 'YN']\n",
    "\n",
    "def load_json_corpus(json_corpus_name:str, load_path=CORPUS_PATH) -> dict:\n",
    "    return joblib.load(os.path.join(load_path, f'{json_corpus_name}.joblib'))\n",
    "\n",
    "def parsejson(text_json:dict):\n",
    "    text_forms = []\n",
    "    text_lemma = []\n",
    "    text_normalised = []\n",
    "    \n",
    "    text_signs = []\n",
    "    text_signs_gdl = []\n",
    "\n",
    "    text_forms_POS = []\n",
    "    text_lemma_POS = []\n",
    "    text_normalised_POS = []\n",
    "\n",
    "    named_entities_in_json = defaultdict(list)\n",
    "\n",
    "    def extract_from_node(obj):\n",
    "        if isinstance(obj, dict):\n",
    "            if obj.get(\"node\") == \"l\" and isinstance(obj.get(\"f\"), dict):\n",
    "                f = obj[\"f\"]\n",
    "\n",
    "                pos  = f.get(\"pos\") or f.get(\"epos\")\n",
    "                form = f.get(\"form\")\n",
    "                lemma = f.get(\"cf\")\n",
    "                norm = f.get(\"norm\") or f.get(\"norm0\")\n",
    "\n",
    "                text_forms.append(form)\n",
    "                text_lemma.append(lemma)\n",
    "                text_normalised.append(norm)\n",
    "\n",
    "                if pos in PN_POSs:\n",
    "                    text_forms_POS.append(f\"PN_{pos}\")\n",
    "                    text_lemma_POS.append(f\"PN_{pos}\")\n",
    "                    text_normalised_POS.append(f\"PN_{pos}\")\n",
    "\n",
    "                    named_entities_in_json[f\"PN_{pos}\"].append(lemma)\n",
    "\n",
    "                else:\n",
    "                    text_forms_POS.append(form)\n",
    "                    text_lemma_POS.append(lemma)\n",
    "                    text_normalised_POS.append(norm)\n",
    "\n",
    "                for g in f.get(\"gdl\", []):\n",
    "                    if isinstance(g, dict):\n",
    "                        if \"v\" in g:\n",
    "                            text_signs.append(g[\"v\"])\n",
    "                        if \"gdl_sign\" in g:\n",
    "                            text_signs_gdl.append(g[\"gdl_sign\"])\n",
    "                        for sub in g.get(\"seq\", []):\n",
    "                            if \"v\" in sub:\n",
    "                                text_signs.append(sub[\"v\"])\n",
    "                            if \"gdl_sign\" in sub:\n",
    "                                text_signs_gdl.append(sub[\"gdl_sign\"])\n",
    "\n",
    "                # for g in f.get(\"gdl\", []):\n",
    "                #     if isinstance(g, dict):\n",
    "                #         if \"v\" in g:\n",
    "                #             text_signs.append(sys.intern(g[\"v\"]))\n",
    "                #         if \"gdl_sign\" in g:\n",
    "                #             text_signs_gdl.append(sys.intern(g[\"gdl_sign\"]))\n",
    "                #         for sub in g.get(\"seq\", []):\n",
    "                #             if \"v\" in sub:\n",
    "                #                 text_signs.append(sys.intern(sub[\"v\"]))\n",
    "                #             if \"gdl_sign\" in sub:\n",
    "                #                 text_signs_gdl.append(sys.intern(sub[\"gdl_sign\"]))\n",
    "\n",
    "            for value in obj.values():\n",
    "                extract_from_node(value)\n",
    "        elif isinstance(obj, list):\n",
    "            for item in obj:\n",
    "                extract_from_node(item)\n",
    "\n",
    "    def change_unknowns(input_list:list):\n",
    "        unknowns = [None, 'x', 'X']\n",
    "        return [\"■\" if item in unknowns else item for item in input_list]\n",
    "\n",
    "    extract_from_node(text_json)\n",
    "\n",
    "    text_forms = change_unknowns(text_forms)\n",
    "    text_lemma = change_unknowns(text_lemma)\n",
    "    text_normalised = change_unknowns(text_normalised)\n",
    "    text_signs = change_unknowns(text_signs)\n",
    "    text_signs_gdl = change_unknowns(text_signs_gdl)\n",
    "    text_forms_POS = change_unknowns(text_forms_POS)\n",
    "    text_lemma_POS = change_unknowns(text_lemma_POS)\n",
    "    text_normalised_POS = change_unknowns(text_normalised_POS)\n",
    "\n",
    "    return {'text_forms': text_forms, 'text_lemma': text_lemma, 'text_normalised': text_normalised, 'text_signs': text_signs, 'text_signs_gdl': text_signs_gdl, 'text_forms_POS': text_forms_POS, 'text_lemma_POS': text_lemma_POS, 'text_normalised_POS': text_normalised_POS, 'text_named_entities': named_entities_in_json}\n",
    "\n",
    "\n",
    "SUB_NUM = str.maketrans('', '', '₀₁₂₃₄₅₆₇₈₉')\n",
    "\n",
    "def normalize_signs(s: str) -> str:\n",
    "    return s.translate(SUB_NUM)\n",
    "\n",
    "def normalize_signs_list(lst: list) -> list:\n",
    "    return [sys.intern(normalize_signs(x) if x is not None else \"■\") for x in lst]\n",
    "\n",
    "class OraccProjectCorpus:\n",
    "    def __init__(self, json_corpus):\n",
    "        self.corpus = json_corpus\n",
    "        self.texts =  [text_id for text_id in json_corpus]\n",
    "        self.texts_data = [json_corpus[text_id] for text_id in json_corpus]\n",
    "        self.size = len(json_corpus)\n",
    "        \n",
    "        analysed_corpus, texts_with_errors, empty_texts = self.AnalyseCorpus()\n",
    "        \n",
    "        self.Lemma = analysed_corpus['lemma']\n",
    "        self.Forms = analysed_corpus['forms']\n",
    "        self.Normalised = analysed_corpus['normalised']\n",
    "        self.Signs = analysed_corpus['signs']\n",
    "        self.SignsGDL = analysed_corpus['signs_gdl']\n",
    "        \n",
    "        self.LemmaPOS = analysed_corpus.get('lemma_POS')\n",
    "        self.FormsPOS = analysed_corpus.get('forms_POS')\n",
    "        self.NormalisedPOS = analysed_corpus.get('normalised_POS')\n",
    "        \n",
    "        self.SignsNormalised = analysed_corpus.get('signs_normalised')\n",
    "        self.FormsNormalised = analysed_corpus.get('forms_normalised')\n",
    "        self.FormsPOSNormalised = analysed_corpus.get('forms_POS_normalised')\n",
    "\n",
    "        self.texts_with_errors = texts_with_errors\n",
    "        self.empty_texts = empty_texts\n",
    "        self.named_entities = analysed_corpus.get('named_entities')\n",
    "\n",
    "    def AnalyseCorpus(self) -> dict: \n",
    "        texts_with_errors = []\n",
    "        empty_texts = []\n",
    "        \n",
    "        corpus_data = {}\n",
    "        \n",
    "        full_corpus_forms = []\n",
    "        full_corpus_lemma = []\n",
    "        full_corpus_normalised = []\n",
    "        full_corpus_signs = []\n",
    "        full_corpus_signs_gdl = []\n",
    "        full_corpus_forms_POS = []\n",
    "        full_corpus_lemma_POS = []\n",
    "        full_corpus_normalised_POS = []\n",
    "        full_corpus_signs_normalised = []\n",
    "        full_corpus_forms_normalised = []\n",
    "        full_corpus_forms_POS_normalised = []\n",
    "        \n",
    "        full_corpus_named_entities = []\n",
    "\n",
    "        # print('\\tAnalyzing texts in the corpus.', self.size, 'texts to be processed.')\n",
    "        \n",
    "        for text_id in self.texts:\n",
    "            \n",
    "            try:\n",
    "                text_analysed = parsejson(self.corpus[text_id])\n",
    "            except:\n",
    "                # TODO: find out the problems with these texts!\n",
    "                #print('ERROR with a text:', text_id)\n",
    "                texts_with_errors.append(text_id)\n",
    "                text_analysed = {'text_forms': [], 'text_lemma': [], 'text_normalised': [], 'text_signs': [], 'text_signs_gdl': [], 'text_forms_POS': [], 'text_lemma_POS': [], 'text_normalised_POS': [], 'signs_normalised': [], 'forms_normalised': [], 'forms_POS_normalised': [], 'named_entities': []}\n",
    "\n",
    "            corpus_data[text_id] = text_analysed\n",
    "            \n",
    "            full_corpus_forms.append(text_analysed['text_forms'])\n",
    "            full_corpus_lemma.append(text_analysed['text_lemma'])\n",
    "            full_corpus_normalised.append(text_analysed['text_normalised'])\n",
    "            full_corpus_signs.append(text_analysed['text_signs'])\n",
    "            full_corpus_signs_gdl.append(text_analysed['text_signs_gdl'])\n",
    "            full_corpus_forms_POS.append(text_analysed['text_forms_POS'])\n",
    "            full_corpus_lemma_POS.append(text_analysed['text_lemma_POS'])\n",
    "            full_corpus_normalised_POS.append(text_analysed['text_normalised_POS'])\n",
    "\n",
    "            full_corpus_signs_normalised.append(normalize_signs_list(text_analysed['text_signs']))\n",
    "            full_corpus_forms_normalised.append(normalize_signs_list(text_analysed['text_forms']))\n",
    "            full_corpus_forms_POS_normalised.append(normalize_signs_list(text_analysed['text_forms_POS']))\n",
    "\n",
    "            full_corpus_named_entities.append(text_analysed['text_named_entities'])\n",
    "\n",
    "            if text_analysed == {'text_forms': [], 'text_lemma': [], 'text_normalised': [], 'text_signs': [], 'text_signs_gdl': [], 'text_forms_POS': [], 'text_lemma_POS': [], 'text_normalised_POS': [], 'signs_normalised': [], 'forms_normalised': [], 'forms_POS_normalised': [], 'named_entities': []}:\n",
    "                empty_texts.append(text_id)\n",
    "\n",
    "        return {'corpus_data': corpus_data, 'forms': full_corpus_forms, 'lemma': full_corpus_lemma, 'normalised': full_corpus_normalised, 'signs': full_corpus_signs, 'signs_gdl': full_corpus_signs_gdl, 'forms_POS': full_corpus_forms_POS, 'lemma_POS': full_corpus_lemma_POS, 'normalised_POS': full_corpus_normalised_POS, 'signs_normalised': full_corpus_signs_normalised, 'forms_normalised': full_corpus_forms_normalised, 'forms_POS_normalised': full_corpus_forms_POS_normalised, 'named_entities': full_corpus_named_entities}, texts_with_errors, empty_texts\n",
    "\n",
    "\n",
    "class OraccCorpus():\n",
    "    def __init__(self, projects_path:str, files_prefix:str='prnd_') -> None: # def __init__(self, input_projects:dict) -> None:\n",
    "        # self.projects = input_projects # not needed, RAM saving\n",
    "\n",
    "        all_texts = []\n",
    "        \n",
    "        lemma_corpus = []\n",
    "        forms_corpus = []\n",
    "        normalised_corpus = []\n",
    "        signs_corpus = []\n",
    "        signs_gdl_corpus = []\n",
    "        forms_POS_corpus = []\n",
    "        lemma_POS_corpus = []\n",
    "        normalised_POS_corpus = []\n",
    "        signs_normalised_corpus = []\n",
    "        forms_normalised_corpus = []\n",
    "        forms_POS_normalised_corpus = []\n",
    "        named_entities_corpus = []\n",
    "\n",
    "        texts_with_errors = []\n",
    "        empty_texts = []\n",
    "    \n",
    "        # for project_name, project_data in tqdm(input_projects.items(), desc='Processing projects'):\n",
    "        # for project_name in tqdm(list(input_projects.keys()), desc='Processing projects...'):\n",
    "        # i=0\n",
    "        for project_file in tqdm(os.listdir(projects_path), desc='Processing project files...'):\n",
    "            # project_data = input_projects.pop(project_name)  # saving RAM (but still working with full dataset --> not good)\n",
    "\n",
    "            # if i >= 10:\n",
    "            #     break\n",
    "\n",
    "            if project_file.startswith(files_prefix) and project_file.endswith('.joblib'):\n",
    "                project_data = load_json_corpus(project_file[:-7], load_path=projects_path)\n",
    "                OPC_project = OraccProjectCorpus(json_corpus=project_data)\n",
    "\n",
    "                for text in OPC_project.Lemma:\n",
    "                    lemma_corpus.append(text)\n",
    "                \n",
    "                for text in OPC_project.Forms:\n",
    "                    forms_corpus.append(text)\n",
    "                    \n",
    "                for text in OPC_project.Normalised:\n",
    "                    normalised_corpus.append(text)\n",
    "\n",
    "                for text in OPC_project.Signs:\n",
    "                    signs_corpus.append(text)\n",
    "                \n",
    "                for text in OPC_project.SignsGDL:\n",
    "                    signs_gdl_corpus.append(text)\n",
    "\n",
    "                for text_id in OPC_project.texts_with_errors:\n",
    "                    texts_with_errors.append(text_id)\n",
    "\n",
    "                for text_id in OPC_project.empty_texts:\n",
    "                    empty_texts.append(text_id)\n",
    "\n",
    "                for text in OPC_project.FormsPOS:\n",
    "                    forms_POS_corpus.append(text)\n",
    "\n",
    "                for text in OPC_project.LemmaPOS:\n",
    "                    lemma_POS_corpus.append(text)\n",
    "\n",
    "                for text in OPC_project.NormalisedPOS:\n",
    "                    normalised_POS_corpus.append(text)\n",
    "\n",
    "                for text_id in OPC_project.texts:\n",
    "                    all_texts.append(text_id)\n",
    "\n",
    "                for text in OPC_project.SignsNormalised:\n",
    "                    signs_normalised_corpus.append(text)\n",
    "\n",
    "                for text in OPC_project.FormsNormalised:\n",
    "                    forms_normalised_corpus.append(text)\n",
    "\n",
    "                for text in OPC_project.FormsPOSNormalised:\n",
    "                    forms_POS_normalised_corpus.append(text)\n",
    "\n",
    "                for text in OPC_project.named_entities:\n",
    "                    named_entities_corpus.append(text)\n",
    "\n",
    "                del project_data, OPC_project # saving RAM\n",
    "                gc.collect() # saving RAM\n",
    "\n",
    "                # i+=1\n",
    "            \n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        print('Corpus size:', len(lemma_corpus), 'texts.')\n",
    "        print('Texts with errors:', len(texts_with_errors), 'texts.')\n",
    "        \n",
    "        for text_id in texts_with_errors:\n",
    "            print('\\t', text_id)\n",
    "\n",
    "        print('Empty texts:', len(empty_texts), 'texts.')\n",
    "\n",
    "        self.texts = all_texts\n",
    "        self.lemma_corpus = lemma_corpus\n",
    "        self.forms_corpus = forms_corpus\n",
    "        self.normalised_corpus = normalised_corpus\n",
    "        \n",
    "        self.signs_corpus = signs_corpus\n",
    "        self.signs_gdl_corpus = signs_gdl_corpus\n",
    "        \n",
    "        self.forms_POS_corpus = forms_POS_corpus\n",
    "        self.lemma_POS_corpus = lemma_POS_corpus\n",
    "        self.normalised_POS_corpus = normalised_POS_corpus\n",
    "\n",
    "        self.signs_normalised_corpus = signs_normalised_corpus\n",
    "        self.forms_normalised_corpus = forms_normalised_corpus\n",
    "        self.forms_POS_normalised_corpus = forms_POS_normalised_corpus\n",
    "\n",
    "        self.texts_with_errors = texts_with_errors\n",
    "        self.empty_texts = empty_texts\n",
    "\n",
    "        self.named_entities = named_entities_corpus\n",
    "\n",
    "\n",
    "    def get_data_by_id(self, text_id, mode='forms', print_=False) -> list:\n",
    "        \"\"\" Print text data for debugging purposes. \"\"\"\n",
    "        try:\n",
    "            txt_idx = self.texts.index(text_id)\n",
    "        except ValueError:\n",
    "            print(f'Text ID {text_id} not found in the corpus.')\n",
    "            return []\n",
    "\n",
    "        if mode == 'forms':\n",
    "            if print_:\n",
    "                print(f'Forms: {self.forms_corpus[txt_idx]}')\n",
    "            return self.forms_corpus[txt_idx]\n",
    "        elif mode == 'lemma':\n",
    "            if print_:\n",
    "                print(f'Lemmas: {self.lemma_corpus[txt_idx]}')\n",
    "            return self.lemma_corpus[txt_idx]\n",
    "        elif mode == 'normalised':\n",
    "            if print_:\n",
    "                print(f'Normalised: {self.normalised_corpus[txt_idx]}')\n",
    "            return self.normalised_corpus[txt_idx]\n",
    "        elif mode == 'signs':\n",
    "            if print_:\n",
    "                print(f'Signs: {self.signs_corpus[txt_idx]}')\n",
    "            return self.signs_corpus[txt_idx]\n",
    "        elif mode == 'signs_gdl':\n",
    "            if print_:\n",
    "                print(f'Signs GDL: {self.signs_gdl_corpus[txt_idx]}')\n",
    "            return self.signs_gdl_corpus[txt_idx]\n",
    "        elif mode == 'forms_pos':\n",
    "            if print_:\n",
    "                print(f'Forms POS: {self.forms_POS_corpus[txt_idx]}')\n",
    "            return self.forms_POS_corpus[txt_idx]\n",
    "        elif mode == 'lemma_pos':\n",
    "            if print_:\n",
    "                print(f'Lemmas POS: {self.lemma_POS_corpus[txt_idx]}')\n",
    "            return self.lemma_POS_corpus[txt_idx]\n",
    "        elif mode == 'normalised_pos':\n",
    "            if print_:\n",
    "                print(f'Normalised POS: {self.normalised_POS_corpus[txt_idx]}')\n",
    "            return self.normalised_POS_corpus[txt_idx]\n",
    "        elif mode == 'signs_normalised':\n",
    "            if print_:\n",
    "                print(f'Signs Normalised: {self.signs_normalised_corpus[txt_idx]}')\n",
    "            return self.signs_normalised_corpus[txt_idx]\n",
    "        elif mode == 'forms_normalised':\n",
    "            if print_:\n",
    "                print(f'Forms Normalised: {self.forms_normalised_corpus[txt_idx]}')\n",
    "            return self.forms_normalised_corpus[txt_idx]\n",
    "        elif mode == 'forms_pos_normalised':\n",
    "            if print_:\n",
    "                print(f'Forms POS Normalised: {self.forms_POS_normalised_corpus[txt_idx]}')\n",
    "            return self.forms_POS_normalised_corpus[txt_idx]\n",
    "        elif mode == 'named_entities':\n",
    "            if print_:\n",
    "                print(f'Named Entities: {self.named_entities[txt_idx]}')\n",
    "            return self.named_entities[txt_idx]\n",
    "        else:\n",
    "            if print_:\n",
    "                print(f'Mode print set wrong! Use \"forms\", \"lemma\", \"normalised\", \"signs\", \"signs_gdl\", \"forms_pos\", \"lemma_pos\", \"normalised_pos\", \"signs_normalised\", \"forms_normalised\", or \"forms_pos_normalised\".')\n",
    "            return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_project_jsons = load_json_corpus('all_projects_jsons')\n",
    "# all_project_jsons_filtered = load_json_corpus('all_projects_jsons_filtered')\n",
    "example_corpus = load_json_corpus('prnd_no_compakklove')  # Load corpus from individual project files\n",
    "for key in example_corpus.keys():\n",
    "    print(key, example_corpus[key])  # Print number of texts in each project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing project files...: 100%|██████████| 156/156 [04:34<00:00,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus size: 28943 texts.\n",
      "Texts with errors: 0 texts.\n",
      "Empty texts: 0 texts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# full_ORACC_corpus = OraccCorpus(all_project_jsons)\n",
    "# full_ORACC_corpus = OraccCorpus(all_project_jsons_filtered)\n",
    "full_ORACC_corpus = OraccCorpus(CORPUS_PATH, files_prefix='prnd_no_comp')  # loading from individual pruned files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_realtions_of_NE(NE_for_analysis:str, category:str, oracc_corpus:OraccCorpus):\n",
    "    # Function to analyze relations of named entities\n",
    "\n",
    "    related_entities = defaultdict(int)\n",
    "    \n",
    "    for text_id in full_ORACC_corpus.texts:\n",
    "        named_entities_in_text = full_ORACC_corpus.get_data_by_id(text_id, mode='named_entities')\n",
    "\n",
    "        if NE_for_analysis in named_entities_in_text[category]:\n",
    "            for ne in set(named_entities_in_text[category]):\n",
    "                if ne != NE_for_analysis:\n",
    "                    related_entities[ne] += 1\n",
    "\n",
    "    return related_entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_en = 'Adad'\n",
    "entity = 'PN_DN'\n",
    "\n",
    "relations_instance = analyse_realtions_of_NE(q_en, entity, oracc_corpus=full_ORACC_corpus)\n",
    "\n",
    "benchmark = 30\n",
    "\n",
    "out_dict = {}\n",
    "out_dict_points = {}\n",
    "i = 0\n",
    "for rel, val in relations_instance.items():\n",
    "    if val >= benchmark:\n",
    "        out_dict[i]={'query_ne': q_en, 'rel_ne': rel, 'hits': val}\n",
    "        out_dict_points[i] = {'point': rel, 'size': val}  # Example point calculation\n",
    "        i += 1\n",
    "\n",
    "out_dict_points[i+1] = {'point': q_en, 'size': len(relations_instance)}  # Example point calculation\n",
    "\n",
    "df_Adad = pd.DataFrame.from_dict(out_dict, orient='index')\n",
    "df_Adad.to_csv(f'flourishdata/{q_en}_relations.csv', sep=',')\n",
    "\n",
    "df_Adad_points = pd.DataFrame.from_dict(out_dict_points, orient='index')\n",
    "df_Adad_points.to_csv(f'flourishdata/{q_en}_relations_points.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities: defaultdict(<class 'list'>, {'PN_CN': ['Arcturus', 'Ṣalbaṭānu', 'Ṣalbaṭānu', 'Ṣalbaṭānu', 'MÚL-KUR-šá-DUR-nu-nu', 'Dilbat', 'DELE-šá-IGI-ABSIN', 'MÚL-ár-šá-SAG-HUN', 'MÚL.MÚL', 'is-le₁₀', 'Ṣalbaṭānu', 'MÚL-KUR-šá-DUR-nu-nu', 'Šihṭu', 'MÚL-IGI-šá-še-pít-MAŠ.MAŠ', 'MAŠ.MAŠ-IGI', 'MÚL-ár-šá-ALLA-šá-ULÙ', 'SAG-A', 'LUGAL', 'GIŠ.KUN-A', 'GÌR-ár-šá-A', 'DELE-šá-IGI-ABSIN', 'Kayyamānu', 'Kakkabu-peṣû', 'Kayyamānu', 'SA₄-šá-ABSIN', 'Dilbat', 'Ṣalbaṭānu', 'Šihṭu', 'RÍN-šá-SI', 'Kakkabu-peṣû', 'Kakkabu-peṣû', 'RÍN-šá-ULÙ', 'Dilbat', 'SA₄-šá-ABSIN', 'Kakkabu-peṣû', 'Zibānītu', 'Dilbat', 'Šerʾu', 'Kayyamānu', 'Zibānītu', 'Ṣalbaṭānu', 'Zibbātu', 'Šihṭu', 'SI-MÁŠ', 'MÚL-IGI-šá-SUHUR-MÁŠ', 'MÚL-KUR-šá-DUR-nu-nu', 'MÚL-IGI-šá-SAG-HUN', 'ŠUR-GIGIR-šá-SI', 'Šihṭu', 'Zuqiqīpu', 'SI₄', 'MAŠ.MAŠ-ár', 'Šihṭu', 'SI₄', 'DELE-šá-IGI-ABSIN', 'Dilbat', 'Kakkabu-peṣû', 'RÍN-šá-ULÙ', 'MÚL-KUR-šá-KIR₄-šil-PA', 'Kakkabu-peṣû', 'Zibānītu', 'Dilbat', 'Zibānītu', 'Zuqiqīpu', 'Šihṭu', 'is-le₁₀', 'ŠUR-GIGIR-šá-SI', 'GIŠ.KUN-A', 'DELE-šá-IGI-ABSIN', 'Šihṭu', 'Dilbat', 'Kakkabu-peṣû', 'Zuqiqīpu', 'Dilbat', 'Pabilsag', 'Suhurmāšu', 'Ṣalbaṭānu', 'MÚL-ár-šá-SAG-HUN', 'Kakkabu-peṣû', 'Dilbat', 'MÚL-IGI-šá-SUHUR-MÁŠ', 'DELE-šá-IGI-ABSIN', 'RÍN-šá-ULÙ', 'Šihṭu', 'Zibbātu', 'SI₄', 'Kakkabu-peṣû', 'Kayyamānu', 'Zuqiqīpu', 'Dilbat', 'Suhurmāšu', 'Gula', 'Šihṭu', 'Zibbātu', 'Ṣalbaṭānu', 'Zappu', 'MÚL.MÚL', 'Ṣalbaṭānu', 'MÚL-IGI-šá-še-pít-MAŠ.MAŠ', 'LUGAL', 'GÌR-ár-šá-A', 'Ṣalbaṭānu', 'ŠUR-GIGIR-šá-SI', 'Kakkabu-peṣû'], 'PN_RN': ['Nabonidus'], 'PN_GN': ['Elamtu', 'Elamtu', 'Elamtu', 'Elamtu', 'Elamtu', 'Elamtu', 'Elamtu', 'Elamtu'], 'PN_MN': ['Arahsamna', 'Ṭebētu', 'Šabāṭu', 'Addaru'], 'PN_DN': ['Adad', 'Adad']})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'PN_CN': ['Arcturus',\n",
       "              'Ṣalbaṭānu',\n",
       "              'Ṣalbaṭānu',\n",
       "              'Ṣalbaṭānu',\n",
       "              'MÚL-KUR-šá-DUR-nu-nu',\n",
       "              'Dilbat',\n",
       "              'DELE-šá-IGI-ABSIN',\n",
       "              'MÚL-ár-šá-SAG-HUN',\n",
       "              'MÚL.MÚL',\n",
       "              'is-le₁₀',\n",
       "              'Ṣalbaṭānu',\n",
       "              'MÚL-KUR-šá-DUR-nu-nu',\n",
       "              'Šihṭu',\n",
       "              'MÚL-IGI-šá-še-pít-MAŠ.MAŠ',\n",
       "              'MAŠ.MAŠ-IGI',\n",
       "              'MÚL-ár-šá-ALLA-šá-ULÙ',\n",
       "              'SAG-A',\n",
       "              'LUGAL',\n",
       "              'GIŠ.KUN-A',\n",
       "              'GÌR-ár-šá-A',\n",
       "              'DELE-šá-IGI-ABSIN',\n",
       "              'Kayyamānu',\n",
       "              'Kakkabu-peṣû',\n",
       "              'Kayyamānu',\n",
       "              'SA₄-šá-ABSIN',\n",
       "              'Dilbat',\n",
       "              'Ṣalbaṭānu',\n",
       "              'Šihṭu',\n",
       "              'RÍN-šá-SI',\n",
       "              'Kakkabu-peṣû',\n",
       "              'Kakkabu-peṣû',\n",
       "              'RÍN-šá-ULÙ',\n",
       "              'Dilbat',\n",
       "              'SA₄-šá-ABSIN',\n",
       "              'Kakkabu-peṣû',\n",
       "              'Zibānītu',\n",
       "              'Dilbat',\n",
       "              'Šerʾu',\n",
       "              'Kayyamānu',\n",
       "              'Zibānītu',\n",
       "              'Ṣalbaṭānu',\n",
       "              'Zibbātu',\n",
       "              'Šihṭu',\n",
       "              'SI-MÁŠ',\n",
       "              'MÚL-IGI-šá-SUHUR-MÁŠ',\n",
       "              'MÚL-KUR-šá-DUR-nu-nu',\n",
       "              'MÚL-IGI-šá-SAG-HUN',\n",
       "              'ŠUR-GIGIR-šá-SI',\n",
       "              'Šihṭu',\n",
       "              'Zuqiqīpu',\n",
       "              'SI₄',\n",
       "              'MAŠ.MAŠ-ár',\n",
       "              'Šihṭu',\n",
       "              'SI₄',\n",
       "              'DELE-šá-IGI-ABSIN',\n",
       "              'Dilbat',\n",
       "              'Kakkabu-peṣû',\n",
       "              'RÍN-šá-ULÙ',\n",
       "              'MÚL-KUR-šá-KIR₄-šil-PA',\n",
       "              'Kakkabu-peṣû',\n",
       "              'Zibānītu',\n",
       "              'Dilbat',\n",
       "              'Zibānītu',\n",
       "              'Zuqiqīpu',\n",
       "              'Šihṭu',\n",
       "              'is-le₁₀',\n",
       "              'ŠUR-GIGIR-šá-SI',\n",
       "              'GIŠ.KUN-A',\n",
       "              'DELE-šá-IGI-ABSIN',\n",
       "              'Šihṭu',\n",
       "              'Dilbat',\n",
       "              'Kakkabu-peṣû',\n",
       "              'Zuqiqīpu',\n",
       "              'Dilbat',\n",
       "              'Pabilsag',\n",
       "              'Suhurmāšu',\n",
       "              'Ṣalbaṭānu',\n",
       "              'MÚL-ár-šá-SAG-HUN',\n",
       "              'Kakkabu-peṣû',\n",
       "              'Dilbat',\n",
       "              'MÚL-IGI-šá-SUHUR-MÁŠ',\n",
       "              'DELE-šá-IGI-ABSIN',\n",
       "              'RÍN-šá-ULÙ',\n",
       "              'Šihṭu',\n",
       "              'Zibbātu',\n",
       "              'SI₄',\n",
       "              'Kakkabu-peṣû',\n",
       "              'Kayyamānu',\n",
       "              'Zuqiqīpu',\n",
       "              'Dilbat',\n",
       "              'Suhurmāšu',\n",
       "              'Gula',\n",
       "              'Šihṭu',\n",
       "              'Zibbātu',\n",
       "              'Ṣalbaṭānu',\n",
       "              'Zappu',\n",
       "              'MÚL.MÚL',\n",
       "              'Ṣalbaṭānu',\n",
       "              'MÚL-IGI-šá-še-pít-MAŠ.MAŠ',\n",
       "              'LUGAL',\n",
       "              'GÌR-ár-šá-A',\n",
       "              'Ṣalbaṭānu',\n",
       "              'ŠUR-GIGIR-šá-SI',\n",
       "              'Kakkabu-peṣû'],\n",
       "             'PN_RN': ['Nabonidus'],\n",
       "             'PN_GN': ['Elamtu',\n",
       "              'Elamtu',\n",
       "              'Elamtu',\n",
       "              'Elamtu',\n",
       "              'Elamtu',\n",
       "              'Elamtu',\n",
       "              'Elamtu',\n",
       "              'Elamtu'],\n",
       "             'PN_MN': ['Arahsamna', 'Ṭebētu', 'Šabāṭu', 'Addaru'],\n",
       "             'PN_DN': ['Adad', 'Adad']})"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_ORACC_corpus.get_data_by_id(full_ORACC_corpus.texts[20], mode='named_entities', print_=True)\n",
    "# full_ORACC_corpus.get_data_by_id(full_ORACC_corpus.texts[20], mode='lemma', print_=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del all_project_jsons\n",
    "# del all_project_jsons_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Validity check \"\"\"\n",
    "\n",
    "# for mode in ['forms', 'lemma', 'normalised', 'signs', 'signs_gdl', 'forms_POS', 'lemma_POS', 'normalised_POS']:\n",
    "#     for t_id in full_ORACC_corpus.texts:\n",
    "#         if full_ORACC_corpus.get_data_by_id(t_id, mode=mode) == full_ORACC_corpus_filtered.get_data_by_id(t_id, mode=mode):\n",
    "#             continue\n",
    "#         else:\n",
    "#             print(\"Mismatch found in text ID:\", t_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intertextuality based on vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\valek\\PthonProjectsOutsideOD\\CuneiformIntertextuality\\CuneiformIntertextuality\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import faiss\n",
    "from typing import List, Tuple, Dict, Any\n",
    "import csv\n",
    "\n",
    "ROOT_PATH = os.getcwd()\n",
    "CHUNKS_PATH = os.path.join(ROOT_PATH, \"chunks\")\n",
    "\n",
    "ORACC_NORM_embed_csv_PATH = os.path.join(CHUNKS_PATH, \"oracc_norm_embeddings.csv\")\n",
    "ORACC_NORM_meta_csv_PATH = os.path.join(CHUNKS_PATH, \"oracc_norm_meta.csv\")\n",
    "\n",
    "ORACC_LEMMA_embed_csv_PATH = os.path.join(CHUNKS_PATH, \"oracc_lemma_embeddings.csv\")\n",
    "ORACC_LEMMA_meta_csv_PATH = os.path.join(CHUNKS_PATH, \"oracc_lemma_meta.csv\")\n",
    "\n",
    "ORACC_FORMS_embed_csv_PATH = os.path.join(CHUNKS_PATH, \"oracc_forms_embeddings.csv\")\n",
    "ORACC_FORMS_meta_csv_PATH = os.path.join(CHUNKS_PATH, \"oracc_forms_meta.csv\")\n",
    "\n",
    "ORACC_NORM_POS_embed_csv_PATH = os.path.join(CHUNKS_PATH, \"oracc_norm_pos_embeddings.csv\")\n",
    "ORACC_NORM_POS_meta_csv_PATH = os.path.join(CHUNKS_PATH, \"oracc_norm_pos_meta.csv\")\n",
    "\n",
    "ORACC_LEMMA_POS_embed_csv_PATH = os.path.join(CHUNKS_PATH, \"oracc_lemma_pos_embeddings.csv\")\n",
    "ORACC_LEMMA_POS_meta_csv_PATH = os.path.join(CHUNKS_PATH, \"oracc_lemma_pos_meta.csv\")\n",
    "\n",
    "ORACC_FORMS_POS_embed_csv_PATH = os.path.join(CHUNKS_PATH, \"oracc_forms_pos_embeddings.csv\")\n",
    "ORACC_FORMS_POS_meta_csv_PATH = os.path.join(CHUNKS_PATH, \"oracc_forms_pos_meta.csv\")\n",
    "\n",
    "ORACC_FORMS_NORMALISED_embed_csv_PATH = os.path.join(CHUNKS_PATH, \"oracc_forms_normalised_embeddings.csv\")\n",
    "ORACC_FORMS_NORMALISED_meta_csv_PATH = os.path.join(CHUNKS_PATH, \"oracc_forms_normalised_meta.csv\")\n",
    "\n",
    "ORACC_FORMS_POS_NORMALISED_embed_csv_PATH = os.path.join(CHUNKS_PATH, \"oracc_forms_pos_normalised_embeddings.csv\")\n",
    "ORACC_FORMS_POS_NORMALISED_meta_csv_PATH = os.path.join(CHUNKS_PATH, \"oracc_forms_pos_normalised_meta.csv\")\n",
    "\n",
    "ORACC_SIGNS_embed_csv_PATH = os.path.join(CHUNKS_PATH, \"oracc_signs_embeddings.csv\")\n",
    "ORACC_SIGNS_meta_csv_PATH = os.path.join(CHUNKS_PATH, \"oracc_signs_meta.csv\")\n",
    "\n",
    "ORACC_SIGNS_NORMALISED_embed_csv_PATH = os.path.join(CHUNKS_PATH, \"oracc_signs_normalised_embeddings.csv\")\n",
    "ORACC_SIGNS_NORMALISED_meta_csv_PATH = os.path.join(CHUNKS_PATH, \"oracc_signs_normalised_meta.csv\")\n",
    "\n",
    "ORACC_SIGNSGDL_embed_csv_PATH = os.path.join(CHUNKS_PATH, \"oracc_signs_gdl_embeddings.csv\")\n",
    "ORACC_SIGNSGDL_meta_csv_PATH = os.path.join(CHUNKS_PATH, \"oracc_signs_gdl_meta.csv\")\n",
    "\n",
    "EMBS_NORM_PATH_E5 = os.path.join(CHUNKS_PATH,\"embeddings_normalised_e5.npy\")\n",
    "EMBS_NORM_POS_PATH_E5 = os.path.join(CHUNKS_PATH,\"embeddings_normalised_pos_e5.npy\")\n",
    "EMBS_LEMMA_PATH_E5 = os.path.join(CHUNKS_PATH,\"embeddings_lemma_e5.npy\")\n",
    "EMBS_LEMMA_POS_PATH_E5 = os.path.join(CHUNKS_PATH,\"embeddings_lemma_pos_e5.npy\")\n",
    "EMBS_FORMS_PATH_E5 = os.path.join(CHUNKS_PATH,\"embeddings_forms_e5.npy\")\n",
    "EMBS_FORMS_NORMALISED_PATH_E5 = os.path.join(CHUNKS_PATH,\"embeddings_forms_normalised_e5.npy\")\n",
    "EMBS_FORMS_POS_PATH_E5 = os.path.join(CHUNKS_PATH,\"embeddings_forms_pos_e5.npy\")\n",
    "EMBS_FORMS_POS_NORMALISED_PATH_E5 = os.path.join(CHUNKS_PATH,\"embeddings_forms_pos_normalised_e5.npy\")\n",
    "EMBS_SIGNS_PATH_E5 = os.path.join(CHUNKS_PATH,\"embeddings_signs_e5.npy\")\n",
    "EMBS_SIGNS_NORMALISED_PATH_E5 = os.path.join(CHUNKS_PATH,\"embeddings_signs_normalised_e5.npy\")\n",
    "EMBS_SIGNSGDL_PATH_E5 = os.path.join(CHUNKS_PATH,\"embeddings_signs_gdl_e5.npy\")\n",
    "\n",
    "EMBS_NORM_PATH_MiniLM = os.path.join(CHUNKS_PATH,\"embeddings_normalised_miniLM.npy\")\n",
    "EMBS_NORM_POS_PATH_MiniLM = os.path.join(CHUNKS_PATH,\"embeddings_normalised_pos_miniLM.npy\")\n",
    "EMBS_LEMMA_PATH_MiniLM = os.path.join(CHUNKS_PATH,\"embeddings_lemma_miniLM.npy\")\n",
    "EMBS_LEMMA_POS_PATH_MiniLM = os.path.join(CHUNKS_PATH,\"embeddings_lemma_pos_miniLM.npy\")\n",
    "EMBS_FORMS_PATH_MiniLM = os.path.join(CHUNKS_PATH,\"embeddings_forms_miniLM.npy\")\n",
    "EMBS_FORMS_POS_PATH_MiniLM = os.path.join(CHUNKS_PATH,\"embeddings_forms_pos_miniLM.npy\")\n",
    "EMBS_FORMS_NORMALISED_PATH_MiniLM = os.path.join(CHUNKS_PATH,\"embeddings_forms_normalised_miniLM.npy\")\n",
    "EMBS_FORMS_POS_NORMALISED_PATH_MiniLM = os.path.join(CHUNKS_PATH,\"embeddings_forms_pos_normalised_miniLM.npy\")\n",
    "EMBS_SIGNS_PATH_MiniLM = os.path.join(CHUNKS_PATH,\"embeddings_signs_miniLM.npy\")\n",
    "EMBS_SIGNS_NORMALISED_PATH_MiniLM = os.path.join(CHUNKS_PATH,\"embeddings_signs_normalised_miniLM.npy\")\n",
    "EMBS_SIGNSGDL_PATH_MiniLM = os.path.join(CHUNKS_PATH,\"embeddings_signs_gdl_miniLM.npy\")\n",
    "\n",
    "IDS_NORM_PATH = os.path.join(CHUNKS_PATH,\"chunk_ids_norm.csv\")\n",
    "IDS_NORM_POS_PATH = os.path.join(CHUNKS_PATH,\"chunk_ids_norm_pos.csv\")\n",
    "IDS_LEMMA_PATH = os.path.join(CHUNKS_PATH,\"chunk_ids_lemma.csv\")\n",
    "IDS_LEMMA_POS_PATH = os.path.join(CHUNKS_PATH,\"chunk_ids_lemma_pos.csv\")\n",
    "IDS_FORMS_PATH = os.path.join(CHUNKS_PATH,\"chunk_ids_forms.csv\")\n",
    "IDS_FORMS_POS_PATH = os.path.join(CHUNKS_PATH,\"chunk_ids_forms_pos.csv\")\n",
    "IDS_FORMS_NORMALISED_PATH = os.path.join(CHUNKS_PATH,\"chunk_ids_forms_normalised.csv\")\n",
    "IDS_FORMS_POS_NORMALISED_PATH = os.path.join(CHUNKS_PATH,\"chunk_ids_forms_pos_normalised.csv\")\n",
    "IDS_SIGNS_PATH = os.path.join(CHUNKS_PATH,\"chunk_ids_signs.csv\")\n",
    "IDS_SIGNS_NORMALISED_PATH = os.path.join(CHUNKS_PATH,\"chunk_ids_signs_normalised.csv\")\n",
    "IDS_SIGNSGDL_PATH = os.path.join(CHUNKS_PATH,\"chunk_ids_signs_gdl.csv\")\n",
    "\n",
    "FAISS_NORM_PATH_E5 = os.path.join(CHUNKS_PATH, \"oracc_norm_e5.faiss\")\n",
    "FAISS_NORM_POS_PATH_E5 = os.path.join(CHUNKS_PATH, \"oracc_norm_pos_e5.faiss\")\n",
    "FAISS_LEMMA_PATH_E5 = os.path.join(CHUNKS_PATH, \"oracc_lemma_e5.faiss\")\n",
    "FAISS_LEMMA_POS_PATH_E5 = os.path.join(CHUNKS_PATH, \"oracc_lemma_pos_e5.faiss\")\n",
    "FAISS_FORMS_PATH_E5 = os.path.join(CHUNKS_PATH, \"oracc_forms_e5.faiss\")\n",
    "FAISS_FORMS_POS_PATH_E5 = os.path.join(CHUNKS_PATH, \"oracc_forms_pos_e5.faiss\")\n",
    "FAISS_FORMS_NORMALISED_PATH_E5 = os.path.join(CHUNKS_PATH, \"oracc_forms_normalised_e5.faiss\")\n",
    "FAISS_FORMS_POS_NORMALISED_PATH_E5 = os.path.join(CHUNKS_PATH, \"oracc_forms_pos_normalised_e5.faiss\")\n",
    "FAISS_SIGNS_PATH_E5 = os.path.join(CHUNKS_PATH, \"oracc_signs_e5.faiss\")\n",
    "FAISS_SIGNS_NORMALISED_PATH_E5 = os.path.join(CHUNKS_PATH, \"oracc_signs_normalised_e5.faiss\")\n",
    "FAISS_SIGNSGDL_PATH_E5 = os.path.join(CHUNKS_PATH, \"oracc_signs_gdl_e5.faiss\")\n",
    "\n",
    "FAISS_NORM_PATH_MiniLM = os.path.join(CHUNKS_PATH, \"oracc_norm_miniLM.faiss\")\n",
    "FAISS_NORM_POS_PATH_MiniLM = os.path.join(CHUNKS_PATH, \"oracc_norm_pos_miniLM.faiss\")\n",
    "FAISS_LEMMA_PATH_MiniLM = os.path.join(CHUNKS_PATH, \"oracc_lemma_miniLM.faiss\")\n",
    "FAISS_LEMMA_POS_PATH_MiniLM = os.path.join(CHUNKS_PATH, \"oracc_lemma_pos_miniLM.faiss\")\n",
    "FAISS_FORMS_PATH_MiniLM = os.path.join(CHUNKS_PATH, \"oracc_forms_miniLM.faiss\")\n",
    "FAISS_FORMS_POS_PATH_MiniLM = os.path.join(CHUNKS_PATH, \"oracc_forms_pos_miniLM.faiss\")\n",
    "FAISS_FORMS_NORMALISED_PATH_MiniLM = os.path.join(CHUNKS_PATH, \"oracc_forms_normalised_miniLM.faiss\")\n",
    "FAISS_FORMS_POS_NORMALISED_PATH_MiniLM = os.path.join(CHUNKS_PATH, \"oracc_forms_pos_normalised_miniLM.faiss\")\n",
    "FAISS_SIGNS_PATH_MiniLM = os.path.join(CHUNKS_PATH, \"oracc_signs_miniLM.faiss\")\n",
    "FAISS_SIGNS_NORMALISED_PATH_MiniLM = os.path.join(CHUNKS_PATH, \"oracc_signs_normalised_miniLM.faiss\")\n",
    "FAISS_SIGNSGDL_PATH_MiniLM = os.path.join(CHUNKS_PATH, \"oracc_signs_gdl_miniLM.faiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_ = 'lala sdsd sdsd sfdsd sd sd  sd sd s sd sd sd sd sd a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['lala', 'sdsd', 'sdsd', 'sfdsd', 'sd'],\n",
       " ['sfdsd', 'sd', 'sd', 'sd', 'sd'],\n",
       " ['sd', 'sd', 's', 'sd', 'sd'],\n",
       " ['sd', 'sd', 'sd', 'sd', 'sd']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_query_text(str_.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking ORACC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_windows(seq: List[str], window: int, stride: int, drop_last: bool=False) -> List[Tuple[int,int,List[str]]]:\n",
    "    \"\"\"\n",
    "    Creates a list of strideping windows from the input sequence.\n",
    "\n",
    "    :param seq: Input sequence (list of tokens/characters)\n",
    "    :param window: Size of the window\n",
    "    :param stride: Stride (step size) for moving the window\n",
    "    :param drop_last: Whether to drop the last window if it's smaller than the specified size\n",
    "    :return: List of tuples (start_idx, end_idx, subseq)\n",
    "    \"\"\"\n",
    "    n = len(seq)\n",
    "    out = []\n",
    "    if n == 0 or window <= 0 or stride <= 0:\n",
    "        return out\n",
    "\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        j = i + window\n",
    "        if j > n:\n",
    "            if drop_last:\n",
    "                break\n",
    "            j = n\n",
    "        out.append((i, j, seq[i:j]))\n",
    "        if j == n:\n",
    "            break\n",
    "        i += stride\n",
    "    return out\n",
    "\n",
    "\n",
    "def chunkORACCtext(input_orrac_corpus: OraccCorpus, oracc_text_ID:str, mode: str='normalised', chunk_size: int=10, stride: int=5, drop_last: bool=False, unknown_policy: str='compress', skip_all_unknown: bool=True) -> List[Dict[str, Any]]:\n",
    "    \"\"\" Parsing ORACC to chunks. \n",
    "\n",
    "    :param input_orrac_corpus: The ORACC corpus object\n",
    "    :param oracc_text_ID: The ID of the ORACC text to process\n",
    "    :param mode: The mode for text retrieval (e.g., 'normalised')\n",
    "    :param chunk_size: The size of each chunk\n",
    "    :param stride: The stride between chunks\n",
    "    :param drop_last: Whether to drop the last chunk if it's smaller than chunk_size\n",
    "    :param unknown_policy: Whether to drop unknown words (select 'compress'|'keep')\n",
    "    :param skip_all_unknown: Whether to skip chunks that are entirely unknown\n",
    "    :return: A tuple containing a list of chunks as tuples (start_idx, end_idx, subseq)\n",
    "    \"\"\"\n",
    "\n",
    "    oraccText = input_orrac_corpus.get_data_by_id(oracc_text_ID, mode=mode)\n",
    "\n",
    "    windows = make_windows(oraccText, window=chunk_size, stride=stride, drop_last=drop_last)\n",
    "\n",
    "    out = []\n",
    "    for s, e, subseq_raw in windows:\n",
    "        # DISPLAY data\n",
    "        text_display = ' '.join(list(subseq_raw))\n",
    "\n",
    "        # EMBEDDING data (UKNOWN handling)\n",
    "        if unknown_policy == 'compress':\n",
    "            emb_tokens = [t for t in subseq_raw if t != '∎']\n",
    "        elif unknown_policy == 'keep':\n",
    "            emb_tokens = list(subseq_raw)\n",
    "        else:\n",
    "            raise ValueError(\"unknown_policy must be 'compress'|'keep'\")\n",
    "\n",
    "        text_embed = ' '.join(emb_tokens).strip()\n",
    "        if skip_all_unknown and text_embed == '' or text_embed == '∎':\n",
    "            continue\n",
    "\n",
    "        out.append({\n",
    "            'start': s,\n",
    "            'end': e,\n",
    "            'text_display': text_display,\n",
    "            'text_embed': text_embed,\n",
    "        })\n",
    "    return out\n",
    "\n",
    "\n",
    "def export_corpus_to_csv(corpus: OraccCorpus, out_embed_csv: str, out_meta_csv: str, mode: str, chunk_size: int = 10, stride: int = 5, drop_last: bool = False, unknown_policy: str='compress', skip_all_unknown: bool=True):\n",
    "\n",
    "    if mode=='normalised':\n",
    "        unit_tag='n'\n",
    "    elif mode=='forms':\n",
    "        unit_tag='f'\n",
    "    elif mode=='forms_normalised':\n",
    "        unit_tag='fn'\n",
    "    elif mode=='lemma':\n",
    "        unit_tag='l'\n",
    "    elif mode=='forms_pos':\n",
    "        unit_tag='fp'\n",
    "    elif mode=='forms_pos_normalised':\n",
    "        unit_tag='fpn'\n",
    "    elif mode=='lemma_pos':\n",
    "        unit_tag='lp'\n",
    "    elif mode=='normalised_pos':\n",
    "        unit_tag='np'\n",
    "    elif mode=='signs':\n",
    "        unit_tag='s'\n",
    "    elif mode=='signs_normalised':\n",
    "        unit_tag='sn'\n",
    "    elif mode=='signs_gdl':\n",
    "        unit_tag='sg'\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown mode: {mode}. Use 'normalised', 'forms', 'lemma', 'normalised_pos', 'forms_pos', 'lemma_pos', 'signs', 'signs_gdl', 'forms_normalised', 'forms_POS_normalised', 'signs_normalised'.\")\n",
    "\n",
    "    with open(out_embed_csv, \"w\", newline=\"\", encoding=\"utf-8\") as fe, \\\n",
    "         open(out_meta_csv,  \"w\", newline=\"\", encoding=\"utf-8\") as fm:\n",
    "        we = csv.writer(fe)\n",
    "        wm = csv.writer(fm)\n",
    "        we.writerow([\"chunk_id\", \"text\"])\n",
    "        wm.writerow([\"chunk_id\", \"text_id\", \"start\", \"end\", \"text_display\"])\n",
    "\n",
    "    total_embed = 0\n",
    "    total_meta = 0\n",
    "\n",
    "    # Streaming text chunks\n",
    "    for textID in tqdm(corpus.texts, desc='Processing ORACC texts'):\n",
    "        recs = chunkORACCtext(\n",
    "            input_orrac_corpus=corpus,\n",
    "            oracc_text_ID=textID,\n",
    "            mode=mode,\n",
    "            chunk_size=chunk_size,\n",
    "            stride=stride,\n",
    "            drop_last=drop_last,\n",
    "            unknown_policy=unknown_policy, \n",
    "            skip_all_unknown=skip_all_unknown\n",
    "        )\n",
    "\n",
    "        with open(out_embed_csv, 'a', newline='', encoding='utf-8') as fe, \\\n",
    "             open(out_meta_csv,  'a', newline='', encoding='utf-8') as fm:\n",
    "            we = csv.writer(fe)\n",
    "            wm = csv.writer(fm)\n",
    "\n",
    "            for r in recs:\n",
    "                chunk_id = f\"{textID}:{unit_tag}:{r['start']}-{r['end']}\"\n",
    "\n",
    "                # meta zapisujeme vždy (aby šlo v UI projít vše, i když embed není)\n",
    "                wm.writerow([chunk_id, textID, r['start'], r['end'], r['text_display']])\n",
    "                total_meta += 1\n",
    "\n",
    "                # do embedding CSV jen neprázdné texty (tvoje logika už ∎ vyhodila)\n",
    "                if r['text_embed']:\n",
    "                    we.writerow([chunk_id, r['text_embed']])\n",
    "                    total_embed += 1\n",
    "\n",
    "    print(f'Saved → {out_embed_csv}: {total_embed} rows')\n",
    "    print(f'Saved → {out_meta_csv}:  {total_meta} rows')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export_corpus_to_csv(corpus=full_ORACC_corpus, out_embed_csv=ORACC_NORM_embed_csv_PATH, out_meta_csv=ORACC_NORM_meta_csv_PATH, mode='normalised')\n",
    "# export_corpus_to_csv(corpus=full_ORACC_corpus, out_embed_csv=ORACC_LEMMA_embed_csv_PATH, out_meta_csv=ORACC_LEMMA_meta_csv_PATH, mode='lemma')\n",
    "# export_corpus_to_csv(corpus=full_ORACC_corpus, out_embed_csv=ORACC_FORMS_embed_csv_PATH, out_meta_csv=ORACC_FORMS_meta_csv_PATH, mode='forms')\n",
    "# export_corpus_to_csv(corpus=full_ORACC_corpus, out_embed_csv=ORACC_FORMS_NORMALISED_embed_csv_PATH, out_meta_csv=ORACC_FORMS_NORMALISED_meta_csv_PATH, mode='forms_normalised')\n",
    "export_corpus_to_csv(corpus=full_ORACC_corpus, out_embed_csv=ORACC_NORM_POS_embed_csv_PATH, out_meta_csv=ORACC_NORM_POS_meta_csv_PATH, mode='normalised_pos')\n",
    "export_corpus_to_csv(corpus=full_ORACC_corpus, out_embed_csv=ORACC_LEMMA_POS_embed_csv_PATH, out_meta_csv=ORACC_LEMMA_POS_meta_csv_PATH, mode='lemma_pos')\n",
    "export_corpus_to_csv(corpus=full_ORACC_corpus, out_embed_csv=ORACC_FORMS_POS_embed_csv_PATH, out_meta_csv=ORACC_FORMS_POS_meta_csv_PATH, mode='forms_pos')\n",
    "export_corpus_to_csv(corpus=full_ORACC_corpus, out_embed_csv=ORACC_FORMS_POS_NORMALISED_embed_csv_PATH, out_meta_csv=ORACC_FORMS_POS_NORMALISED_meta_csv_PATH, mode='forms_pos_normalised')\n",
    "# export_corpus_to_csv(corpus=full_ORACC_corpus, out_embed_csv=ORACC_SIGNS_embed_csv_PATH, out_meta_csv=ORACC_SIGNS_meta_csv_PATH, mode='signs', chunk_size=25, stride=10)\n",
    "# export_corpus_to_csv(corpus=full_ORACC_corpus, out_embed_csv=ORACC_SIGNS_NORMALISED_embed_csv_PATH, out_meta_csv=ORACC_SIGNS_NORMALISED_meta_csv_PATH, mode='signs_normalised', chunk_size=25, stride=10)\n",
    "# export_corpus_to_csv(corpus=full_ORACC_corpus, out_embed_csv=ORACC_SIGNSGDL_embed_csv_PATH, out_meta_csv=ORACC_SIGNSGDL_meta_csv_PATH, mode='signs_gdl', chunk_size=25, stride=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def batched(iterable, n):\n",
    "    for i in range(0, len(iterable), n):\n",
    "        yield iterable[i:i+n]\n",
    "\n",
    "\n",
    "def process_chunks(input_csv_path:str, output_embeddings_path:str, output_ids_path:str, model_name:str, device:str='cuda', batch_size:int=2048):\n",
    "    df = pd.read_csv(input_csv_path)\n",
    "    ids = df['chunk_id'].astype(str).tolist()\n",
    "    texts = df['text'].astype(str).tolist()\n",
    "\n",
    "    model = SentenceTransformer(model_name, device=device)\n",
    "    all_vecs = []\n",
    "\n",
    "    total = (len(texts) + batch_size - 1) // batch_size\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch_size), total=total):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        vecs = model.encode(batch, batch_size=batch_size, normalize_embeddings=True, show_progress_bar=False).astype('float32')\n",
    "        \n",
    "        all_vecs.append(vecs)\n",
    "\n",
    "    E = np.vstack(all_vecs)\n",
    "    np.save(output_embeddings_path, E)\n",
    "    pd.Series(ids, name='chunk_id').to_csv(output_ids_path, index=False)\n",
    "\n",
    "    print(\"Saved to:\", output_embeddings_path, E.shape, \" / \", output_ids_path, len(ids))\n",
    "\n",
    "def process_chunks_POS(input_csv_path:str, output_embeddings_path:str, output_ids_path:str,\n",
    "                   model_name='intfloat/e5-base-v2', device:str='cuda', batch_size:int=2048,\n",
    "                   text_prefix:str='passage: '):\n",
    "    df = pd.read_csv(input_csv_path)\n",
    "    ids = df[\"chunk_id\"].astype(str).tolist()\n",
    "    texts = df[\"text\"].astype(str).tolist()\n",
    "\n",
    "    if text_prefix:\n",
    "        texts = [f\"{text_prefix}{t}\" for t in texts]\n",
    "\n",
    "    model = SentenceTransformer(model_name, device=device)\n",
    "\n",
    "    model.max_seq_length = 96       # (zkus 96; když chceš víc přesnosti, dej 128)\n",
    "    if device == \"cuda\":\n",
    "        model = model.to(torch.float16)\n",
    "        torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "    all_vecs = []\n",
    "    total = (len(texts) + batch_size - 1) // batch_size\n",
    "    for i in tqdm(range(0, len(texts), batch_size), total=total):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        vecs = model.encode(batch, batch_size=batch_size,\n",
    "                            normalize_embeddings=True, show_progress_bar=False\n",
    "                           ).astype(\"float32\")\n",
    "        all_vecs.append(vecs)\n",
    "    E = np.vstack(all_vecs)\n",
    "    np.save(output_embeddings_path, E)\n",
    "    pd.Series(ids, name=\"chunk_id\").to_csv(output_ids_path, index=False)\n",
    "\n",
    "    print(\"Uloženo:\", output_embeddings_path, E.shape, \" / \", output_ids_path, len(ids))\n",
    "\n",
    "def select_paths(mode='normalised', model='e5'):\n",
    "    if mode == 'normalised':\n",
    "        if model == 'e5':\n",
    "            return (ORACC_NORM_embed_csv_PATH, EMBS_NORM_PATH_E5, IDS_NORM_PATH, FAISS_NORM_PATH_E5, 'intfloat/e5-base-v2', ORACC_NORM_meta_csv_PATH)\n",
    "        elif model == 'MiniLM':\n",
    "            return (ORACC_NORM_embed_csv_PATH, EMBS_NORM_PATH_MiniLM, IDS_NORM_PATH, FAISS_NORM_PATH_MiniLM, 'all-MiniLM-L6-v2', ORACC_NORM_meta_csv_PATH)\n",
    "    elif mode == 'normalised_POS':\n",
    "        if model == 'e5':\n",
    "            return (ORACC_NORM_POS_embed_csv_PATH, EMBS_NORM_POS_PATH_E5, IDS_NORM_POS_PATH, FAISS_NORM_POS_PATH_E5, 'intfloat/e5-base-v2', ORACC_NORM_POS_meta_csv_PATH)\n",
    "        elif model == 'MiniLM':\n",
    "            return (ORACC_NORM_POS_embed_csv_PATH, EMBS_NORM_POS_PATH_MiniLM, IDS_NORM_POS_PATH, FAISS_NORM_POS_PATH_MiniLM, 'all-MiniLM-L6-v2', ORACC_NORM_POS_meta_csv_PATH)\n",
    "    elif mode == 'lemma':\n",
    "        if model == 'e5':\n",
    "            return (ORACC_LEMMA_embed_csv_PATH, EMBS_LEMMA_PATH_E5, IDS_LEMMA_PATH, FAISS_LEMMA_PATH_E5, 'intfloat/e5-base-v2', ORACC_LEMMA_meta_csv_PATH)\n",
    "        elif model == 'MiniLM':\n",
    "            return (ORACC_LEMMA_embed_csv_PATH, EMBS_LEMMA_PATH_MiniLM, IDS_LEMMA_PATH, FAISS_LEMMA_PATH_MiniLM, 'all-MiniLM-L6-v2', ORACC_LEMMA_meta_csv_PATH)\n",
    "    elif mode == 'lemma_POS':\n",
    "        if model == 'e5':\n",
    "            return (ORACC_LEMMA_POS_embed_csv_PATH, EMBS_LEMMA_POS_PATH_E5, IDS_LEMMA_POS_PATH, FAISS_LEMMA_POS_PATH_E5, 'intfloat/e5-base-v2', ORACC_LEMMA_POS_meta_csv_PATH)\n",
    "        elif model == 'MiniLM':\n",
    "            return (ORACC_LEMMA_POS_embed_csv_PATH, EMBS_LEMMA_POS_PATH_MiniLM, IDS_LEMMA_POS_PATH, FAISS_LEMMA_POS_PATH_MiniLM, 'all-MiniLM-L6-v2', ORACC_LEMMA_POS_meta_csv_PATH)\n",
    "    elif mode == 'forms':\n",
    "        if model == 'e5':\n",
    "            return (ORACC_FORMS_embed_csv_PATH, EMBS_FORMS_PATH_E5, IDS_FORMS_PATH, FAISS_FORMS_PATH_E5, 'intfloat/e5-base-v2', ORACC_FORMS_meta_csv_PATH)\n",
    "        elif model == 'MiniLM':\n",
    "            return (ORACC_FORMS_embed_csv_PATH, EMBS_FORMS_PATH_MiniLM, IDS_FORMS_PATH, FAISS_FORMS_PATH_MiniLM, 'all-MiniLM-L6-v2', ORACC_FORMS_meta_csv_PATH)\n",
    "    elif mode == 'forms_POS':\n",
    "        if model == 'e5':\n",
    "            return (ORACC_FORMS_POS_embed_csv_PATH, EMBS_FORMS_POS_PATH_E5, IDS_FORMS_POS_PATH, FAISS_FORMS_POS_PATH_E5, 'intfloat/e5-base-v2', ORACC_FORMS_POS_meta_csv_PATH)\n",
    "        elif model == 'MiniLM':\n",
    "            return (ORACC_FORMS_POS_embed_csv_PATH, EMBS_FORMS_POS_PATH_MiniLM, IDS_FORMS_POS_PATH, FAISS_FORMS_POS_PATH_MiniLM, 'all-MiniLM-L6-v2', ORACC_FORMS_POS_meta_csv_PATH)\n",
    "    elif mode == 'signs':\n",
    "        if model == 'e5':\n",
    "            return (ORACC_SIGNS_embed_csv_PATH, EMBS_SIGNS_PATH_E5, IDS_SIGNS_PATH, FAISS_SIGNS_PATH_E5, 'intfloat/e5-base-v2', ORACC_SIGNS_meta_csv_PATH)\n",
    "        elif model == 'MiniLM':\n",
    "            return (ORACC_SIGNS_embed_csv_PATH, EMBS_SIGNS_PATH_MiniLM, IDS_SIGNS_PATH, FAISS_SIGNS_PATH_MiniLM, 'all-MiniLM-L6-v2', ORACC_SIGNS_meta_csv_PATH)\n",
    "    elif mode == 'signs_gdl':\n",
    "        if model == 'e5':\n",
    "            return (ORACC_SIGNSGDL_embed_csv_PATH, EMBS_SIGNSGDL_PATH_E5, IDS_SIGNSGDL_PATH, FAISS_SIGNSGDL_PATH_E5, 'intfloat/e5-base-v2', ORACC_SIGNSGDL_meta_csv_PATH)\n",
    "        elif model == 'MiniLM':\n",
    "            return (ORACC_SIGNSGDL_embed_csv_PATH, EMBS_SIGNSGDL_PATH_MiniLM, IDS_SIGNSGDL_PATH, FAISS_SIGNSGDL_PATH_MiniLM, 'all-MiniLM-L6-v2', ORACC_SIGNSGDL_meta_csv_PATH)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown mode: \" + mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for mode in ['forms', 'normalised', 'lemma', 'signs', 'signs_gdl']:\n",
    "#     paths = select_paths(mode=mode)\n",
    "\n",
    "#     process_chunks(\n",
    "#         input_csv_path=paths[0],\n",
    "#         output_embeddings_path=paths[1],\n",
    "#         output_ids_path=paths[2],\n",
    "#         model_name=MODEL_NAME,\n",
    "#         device=\"cuda\",\n",
    "#         batch_size=BATCH_SIZE\n",
    "#     )\n",
    "\n",
    "# NOTE: this has been run in chunk.py\n",
    "# for mode in ['forms_POS', 'normalised_POS', 'lemma_POS']:\n",
    "    \n",
    "#     paths = select_paths(mode=mode)\n",
    "\n",
    "#     process_chunks_POS(\n",
    "#         input_csv_path=paths[0],\n",
    "#         output_embeddings_path=paths[1],\n",
    "#         output_ids_path=paths[2],\n",
    "#         model_name='intfloat/e5-base-v2',\n",
    "#         device=\"cuda\",\n",
    "#         batch_size=2048\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_FAISS(input_embeddings_path:str, output_faiss_path:str, nlist:int=1024):\n",
    "\n",
    "    MIN_NLIST, MAX_NLIST = 256, 32768\n",
    "    TRAIN_MULTIPLIER = 128\n",
    "\n",
    "    print(\"Načítám embeddingy…\")\n",
    "    E = np.load(input_embeddings_path).astype(\"float32\")\n",
    "    N, d = E.shape\n",
    "    print(f\"Vektorů: {N}, dim: {d}\")\n",
    "\n",
    "    # odhad nlist\n",
    "    nlist = int(4 * (N ** 0.5))\n",
    "    nlist = max(MIN_NLIST, min(nlist, MAX_NLIST))\n",
    "    print(f\"nlist = {nlist}\")\n",
    "\n",
    "    # kvantizér pro IP (embeddings jsou už normalizované → IP = cosine)\n",
    "    quantizer = faiss.IndexFlatIP(d)\n",
    "    index = faiss.IndexIVFFlat(quantizer, d, nlist, faiss.METRIC_INNER_PRODUCT)\n",
    "\n",
    "    # vybrat trénovací vzorek\n",
    "    train_size = min(N, nlist * TRAIN_MULTIPLIER)\n",
    "    print(f\"Tréninkový vzorek: {train_size}\")\n",
    "    rng = np.random.default_rng(42)\n",
    "    train_idx = rng.choice(N, size=train_size, replace=False)\n",
    "    train_vecs = E[train_idx]\n",
    "\n",
    "    print(\"Trénuji IVF…\")\n",
    "    index.train(train_vecs)\n",
    "    assert index.is_trained\n",
    "\n",
    "    print(\"Přidávám vektory do indexu…\")\n",
    "    index.add(E)   # lze i po dávkách, ale add() si data zkopíruje\n",
    "\n",
    "    faiss.write_index(index, output_faiss_path)\n",
    "    print(f\"Hotovo. Uložen index: {output_faiss_path}  |  ntotal={index.ntotal}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mode in ['normalised', 'normalised_POS', 'lemma', 'lemma_POS', 'forms', 'forms_POS', 'signs', 'signs_gdl']:\n",
    "    for model in ['e5', 'MiniLM']:\n",
    "        paths = select_paths(mode=mode, model=model)\n",
    "\n",
    "        make_FAISS(input_embeddings_path=paths[1], output_faiss_path=paths[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import unicodedata\n",
    "# from rapidfuzz import fuzz\n",
    "# from sentence_transformers import CrossEncoder\n",
    "\n",
    "# INDEX_PATH = FAISS_NORM_POS_PATH\n",
    "# IDS_PATH   = IDS_NORM_POS_PATH\n",
    "# META_PATH  = ORACC_NORM_POS_meta_csv_PATH\n",
    "\n",
    "# MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "\n",
    "# TOPK = 20\n",
    "# NPROBE = 64\n",
    "\n",
    "# # ---- načtení indexu, id mapy a metadat ----\n",
    "# index = faiss.read_index(INDEX_PATH)\n",
    "# index.nprobe = NPROBE\n",
    "\n",
    "# ids = pd.read_csv(IDS_PATH)[\"chunk_id\"].astype(str).tolist()\n",
    "# meta = pd.read_csv(META_PATH).astype({\"chunk_id\":\"string\"})\n",
    "\n",
    "# device = \"cuda\"\n",
    "# model = SentenceTransformer(MODEL_NAME, device=device)\n",
    "\n",
    "# ce = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from rapidfuzz import fuzz\n",
    "import unicodedata\n",
    "\n",
    "# def _strip_diac(s: str) -> str:\n",
    "#     return \"\".join(ch for ch in unicodedata.normalize(\"NFD\", s) if unicodedata.category(ch) != \"Mn\")\n",
    "\n",
    "def query_to_pos(query: str) -> str:\n",
    "    out = []\n",
    "    for tok in query.split():\n",
    "        # logogramy celé VELKÉ necháme být\n",
    "        if tok.isupper():\n",
    "            out.append(tok)\n",
    "            continue\n",
    "        # heuristika na jména: aspoň jeden segment začíná velkým písmenem\n",
    "        segs = tok.split(\"-\")\n",
    "        if any(seg and seg[0].isalpha() and seg[0].isupper() for seg in segs):\n",
    "            out.append(\"PN_RN\")\n",
    "        else:\n",
    "            out.append(tok)  # zachovat diakritiku i case\n",
    "    return \" \".join(out)\n",
    "\n",
    "\n",
    "\"\"\" E5 ------------------------------------------ \"\"\"\n",
    "\n",
    "\n",
    "_POS_BASE = {'an', 'cn', 'dn', 'en', 'fn', 'gn', 'ln', 'mn', 'on', 'pn', 'qn', 'rn', 'sn', 'tn', 'wn', 'yn'}\n",
    "\n",
    "# def _is_pos_tag_token(tok: str) -> bool:\n",
    "#     parts = tok.lower().split(\"_\")\n",
    "#     return all(p in _POS_BASE for p in parts)\n",
    "\n",
    "# def _tokenize(s: str) -> list[str]:\n",
    "#     # Unicode slova (vč. diakritiky a underscore), bez čistě číselných tokenů\n",
    "#     toks = re.findall(r\"\\w+\", str(s), flags=re.UNICODE)\n",
    "#     return [t.lower() for t in toks if not t.isdigit()]\n",
    "\n",
    "# def _weighted_overlap(q: str, t: str, tag_w: float = 0.3) -> float:\n",
    "#     \"\"\"\n",
    "#     Vážený překryv tokenů (0..1):\n",
    "#       - obsahové tokeny (ne-POS) váha 1.0\n",
    "#       - POS tagy (PN_RN apod.) váha tag_w (např. 0.3)\n",
    "#     \"\"\"\n",
    "#     q_toks = set(_tokenize(q))\n",
    "#     t_toks = set(_tokenize(t))\n",
    "\n",
    "#     def w(tok: str) -> float:\n",
    "#         return tag_w if _is_pos_tag_token(tok) else 1.0\n",
    "\n",
    "#     # vážený součet přes *dotazové* tokeny\n",
    "#     denom = sum(w(tok) for tok in q_toks) or 1.0\n",
    "#     num   = sum(w(tok) for tok in q_toks if tok in t_toks)\n",
    "#     return num / denom\n",
    "\n",
    "def _weighted_overlap_POS(q: str, t: str, tag_w: float = 0.3) -> float:\n",
    "    def is_tag(tok): return all(p in _POS_BASE for p in tok.split(\"_\"))\n",
    "    def toks(s): \n",
    "        ts = re.findall(r\"\\w+\", str(s), flags=re.UNICODE)\n",
    "        return [t.lower() for t in ts if not t.isdigit()]\n",
    "    q_toks, t_toks = set(toks(q)), set(toks(t))\n",
    "    def w(tok): return (tag_w if is_tag(tok) else 1.0)\n",
    "    denom = sum(w(t) for t in q_toks) or 1.0\n",
    "    num   = sum(w(t) for t in q_toks if t in t_toks)\n",
    "    return num / denom\n",
    "\n",
    "def search_query_e5(query: str, faiss_idx_path:str, ids_path:str, meta_csv_path:str, topk: int = 10, nprobe: int = 256, cand: int = 2000) -> pd.DataFrame:\n",
    "    # Loading E5 model\n",
    "    e5 = SentenceTransformer('intfloat/e5-base-v2', device=\"cuda\")\n",
    "    e5.max_seq_length = 96\n",
    "    e5 = e5.to(torch.float16)\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "    # --- načti POS index/ids/meta postavené na E5 embeddingách ---\n",
    "    index_pos_e5 = faiss.read_index(faiss_idx_path)\n",
    "    ids_pos = pd.read_csv(ids_path)[\"chunk_id\"].astype(str).tolist()\n",
    "    meta_pos = pd.read_csv(meta_csv_path).astype({\"chunk_id\":\"string\"})\n",
    "    \n",
    "    \n",
    "    # 1) převede dotaz do POS tvaru, zachová diakritiku/case u obsahu\n",
    "    q_pos = query_to_pos(query)             # např. \"PN_RN rīm tuqumtim\"\n",
    "    # 2) E5: dotaz musí mít prefix \"query: \"\n",
    "    q_emb = e5.encode([f\"query: {q_pos}\"], normalize_embeddings=True).astype(\"float32\")\n",
    "    # 3) FAISS\n",
    "    index_pos_e5.nprobe = nprobe\n",
    "    D, I = index_pos_e5.search(q_emb, max(topk, cand))\n",
    "    hit_ids = [ids_pos[i] for i in I[0]]\n",
    "    embed_s = [float(s) for s in D[0]]\n",
    "\n",
    "    df = pd.DataFrame({\"chunk_id\": hit_ids, \"embed_score\": embed_s}).merge(meta_pos, on=\"chunk_id\", how=\"left\")\n",
    "    \n",
    "    # malý strukturální signál z POS řetězce (slabá váha)\n",
    "    q_pos_lower = q_pos.lower()\n",
    "    df[\"lex\"] = df[\"text_display\"].astype(str).str.lower().apply(lambda t: _weighted_overlap_POS(q_pos_lower, t, tag_w=0.3))\n",
    "    df[\"score\"] = 0.9*df[\"embed_score\"] + 0.1*df[\"lex\"]\n",
    "\n",
    "    df = df.drop_duplicates(\"chunk_id\").sort_values(\"score\", ascending=False).head(topk).reset_index(drop=True)\n",
    "    df.insert(0, \"rank\", range(1, len(df)+1))\n",
    "    df[\"text_display\"] = df[\"text_display\"].astype(str).str.slice(0, 180)\n",
    "    return df[[\"rank\",\"score\",\"embed_score\",\"lex\",\"chunk_id\",\"text_id\",\"start\",\"end\",\"text_display\"]]\n",
    "\n",
    "\n",
    "\"\"\" MINI LM ------------------------------------ \"\"\"\n",
    "def _lex_sim(query: str, text: str) -> float:\n",
    "    q = str(query).lower()\n",
    "    t = str(text).lower().replace(\"∎\", \" \")\n",
    "    return max(fuzz.partial_ratio(q, t), fuzz.token_set_ratio(q, t)) / 100.0\n",
    "\n",
    "def search_query_MiniLM(query: str, faiss_idx_path:str, ids_path:str, meta_path:str, topk: int = 10, nprobe: int = 128, cand: int = 1000) -> pd.DataFrame:\n",
    "    \n",
    "    mini = SentenceTransformer(\"all-MiniLM-L6-v2\", device='cuda')\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "    index_norm = faiss.read_index(faiss_idx_path)\n",
    "    ids_norm   = pd.read_csv(ids_path)[\"chunk_id\"].astype(str).tolist()\n",
    "    meta_norm  = pd.read_csv(meta_path).astype({\"chunk_id\":\"string\"})\n",
    "    meta_norm[\"text_display\"] = meta_norm[\"text_display\"].astype(str)\n",
    "\n",
    "    # 1) FAISS kandidáti\n",
    "    index_norm.nprobe = nprobe\n",
    "    q_emb = mini.encode([query], normalize_embeddings=True).astype(\"float32\")\n",
    "    D, I = index_norm.search(q_emb, max(topk, cand))\n",
    "\n",
    "    # 2) Poskládat výsledky + meta\n",
    "    hit_ids  = [ids_norm[i] for i in I[0]]\n",
    "    embed_s  = [float(s) for s in D[0]]\n",
    "    df = pd.DataFrame({\"chunk_id\": hit_ids, \"embed_score\": embed_s}).drop_duplicates(\"chunk_id\")\n",
    "    df = df.merge(meta_norm, on=\"chunk_id\", how=\"left\")\n",
    "\n",
    "    # 3) Lehký lexikální rerank\n",
    "    df[\"lex\"] = df[\"text_display\"].apply(lambda t: _lex_sim(query, t))\n",
    "    df[\"score\"] = 0.7 * df[\"embed_score\"] + 0.3 * df[\"lex\"]\n",
    "\n",
    "    # 4) Řazení a výstup\n",
    "    df = df.sort_values(\"score\", ascending=False).head(topk).reset_index(drop=True)\n",
    "    df.insert(0, \"rank\", range(1, len(df)+1))\n",
    "    df[\"text_display\"] = df[\"text_display\"].str.slice(0, 180)\n",
    "    return df[[\"rank\",\"score\",\"embed_score\",\"lex\",\"chunk_id\",\"text_id\",\"start\",\"end\",\"text_display\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ORACC_corpus.get_data_by_id('nere/Q009326', mode='lemma')[:6] # Example of getting data by text ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Comparing models and their results. \"\"\"\n",
    "\n",
    "results = {}\n",
    "\n",
    "for mode in ['normalised', 'normalised_POS', 'lemma', 'lemma_POS', 'forms', 'forms_POS', 'signs', 'signs_gdl']:\n",
    "    query_len = 6\n",
    "    if mode in ['signs', 'signs_gdl']:\n",
    "        query_len = 20\n",
    "\n",
    "    query = ' '.join(full_ORACC_corpus.get_data_by_id('nere/Q009326', mode=mode)[:query_len])\n",
    "    \n",
    "    for model in ['e5', 'MiniLM']:\n",
    "        print(f\"Searching for query: {query} in mode: {mode} with model: {model}\")\n",
    "        paths = select_paths(mode=mode, model=model)\n",
    "\n",
    "        model_name = paths[4]  # model name\n",
    "        faiss_idx_path = paths[3]  # FAISS index path\n",
    "        ids_path = paths[2]  # IDs path\n",
    "        meta_csv_path = paths[5]\n",
    "\n",
    "        if model == 'e5':\n",
    "            search_results = search_query_e5(query, faiss_idx_path, ids_path, meta_csv_path)\n",
    "\n",
    "            results[(query, mode, model)] = search_results\n",
    "\n",
    "        elif model == 'MiniLM':\n",
    "            search_results = search_query_MiniLM(query, faiss_idx_path, ids_path, meta_csv_path)\n",
    "\n",
    "            results[(query, mode, model)] = search_results\n",
    "\n",
    "        else:\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for res in results:\n",
    "    query, mode, model = res\n",
    "    print(f\"Results for {mode} with {model}:\")\n",
    "    print(results[res].head(10))  # Display top 10 results\n",
    "    print(\"\\n\")  # New line for better readability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct Intertextuality (with edit distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "ROOT_PATH = os.getcwd()\n",
    "\n",
    "DIRECT_INTERTEXT_METADATA_PATH = os.path.join(ROOT_PATH, \"directintertext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from rapidfuzz.distance import Levenshtein\n",
    "\n",
    "def token_edit_distance_inner(query: List[str], target: List[str], max_total_ed: Optional[int] = None, unknown_token: str = 'UNKNOWN'):\n",
    "    m, n = len(query), len(target)\n",
    "    if m == 0 or m > n:\n",
    "        return None\n",
    "\n",
    "    best_sum: Optional[int] = None\n",
    "    best_hits: List[Tuple[int, int, List[str]]] = []\n",
    "\n",
    "    for i in range(0, n - m + 1):\n",
    "        s = 0\n",
    "        for j in range(m):\n",
    "            if query[j] == unknown_token:\n",
    "                d = 0\n",
    "            else:\n",
    "                d = Levenshtein.distance(query[j], target[i + j])\n",
    "            \n",
    "            s += d\n",
    "            \n",
    "            # DŮLEŽITÉ: přeruš jen když je PRŮBĚŽNÁ suma HORŠÍ, ne shodná\n",
    "            if best_sum is not None and s > best_sum:\n",
    "                break\n",
    "        else:\n",
    "            if best_sum is None or s < best_sum:\n",
    "                best_sum = s\n",
    "                best_hits = [(s, i, target[i:i+m])]\n",
    "            elif s == best_sum:\n",
    "                best_hits.append((s, i, target[i:i+m]))\n",
    "\n",
    "    if best_sum is None:\n",
    "        return None\n",
    "    if max_total_ed is not None and best_sum > max_total_ed:\n",
    "        return None\n",
    "\n",
    "    return best_hits\n",
    "\n",
    "def token_edit_distance(query: List[str], target: List[str], max_total_ed: Optional[int] = None, unknown_token: str = 'UNKNOWN') -> Optional[int]:\n",
    "    \"\"\" This function serves to search for hits, considering edit distance on the level of full tokens. (e.g., 'inuma ilu ibnu awilutam' // 'inuma blabla ibnu awilutam' has edit distance 1) \"\"\"\n",
    "    m, n = len(query), len(target)\n",
    "    if m == 0 or n == 0:\n",
    "        return None\n",
    "\n",
    "    # DP a backtrack\n",
    "    dp = [[0]*(n+1) for _ in range(m+1)]\n",
    "    bt = [[0]*(n+1) for _ in range(m+1)]  # 0=diag, 1=up(delete), 2=left(insert)\n",
    "\n",
    "    for j in range(n+1):\n",
    "        dp[0][j] = 0                 # substring může začít kdekoliv\n",
    "    for i in range(1, m+1):\n",
    "        dp[i][0] = i                 # smazání i tokenů z query\n",
    "        bt[i][0] = 1\n",
    "\n",
    "    for i in range(1, m+1):\n",
    "        ai = query[i-1]\n",
    "        for j in range(1, n+1):\n",
    "            bj = target[j-1]\n",
    "            cost = 0 if (ai == bj or ai == unknown_token) else 1\n",
    "            del_q = dp[i-1][j] + 1        # delete ai\n",
    "            ins_q = dp[i][j-1] + 1        # insert bj\n",
    "            sub  = dp[i-1][j-1] + cost    # match/replace (UNKNOWN matchuje cokoliv za 0)\n",
    "\n",
    "            if sub <= del_q and sub <= ins_q:\n",
    "                dp[i][j] = sub; bt[i][j] = 0\n",
    "            elif del_q <= ins_q:\n",
    "                dp[i][j] = del_q; bt[i][j] = 1\n",
    "            else:\n",
    "                dp[i][j] = ins_q; bt[i][j] = 2\n",
    "\n",
    "    best = min(dp[m][1:])  # nejlepší vzdálenost přes všechna možná zakončení\n",
    "    if max_total_ed is not None and best > max_total_ed:\n",
    "        return None\n",
    "\n",
    "    hits: List[Tuple[int, int, int, List[str]]] = []\n",
    "    for j in range(1, n+1):\n",
    "        if dp[m][j] != best:\n",
    "            continue\n",
    "        # backtrack z (m, j) na začátek substringu (řádek i==0)\n",
    "        i, jj = m, j\n",
    "        while i > 0:\n",
    "            move = bt[i][jj]\n",
    "            if move == 0:      # diag\n",
    "                i -= 1; jj -= 1\n",
    "            elif move == 1:    # up (delete v query)\n",
    "                i -= 1\n",
    "            else:              # left (insert v targetu)\n",
    "                jj -= 1\n",
    "        start, end = jj, j\n",
    "        hits.append((best, start, end, target[start:end]))\n",
    "\n",
    "    return hits if hits else None\n",
    "\n",
    "    # NOTE: before adding unknown token filter:\n",
    "    # m, n = len(query), len(target)\n",
    "    # if m == 0 or n == 0:\n",
    "    #     return None\n",
    "\n",
    "    # # DP matice (m+1) x (n+1); dp[i][j] = min ed mezi query[:i] a target[:j]\n",
    "    # # \"Substring trick\": dovolíme začátek substringu kdekoliv nastavením dp[0][j] = 0 pro všechna j\n",
    "    # dp = [[0]*(n+1) for _ in range(m+1)]\n",
    "    # bt = [[0]*(n+1) for _ in range(m+1)]  # backtrack: 0=diag, 1=up(delete in query), 2=left(insert)\n",
    "\n",
    "    # for j in range(n+1):\n",
    "    #     dp[0][j] = 0  # klíč: začátek substringu může být kdekoliv\n",
    "    # for i in range(1, m+1):\n",
    "    #     dp[i][0] = i  # musíme smazat i tokenů z query, když je target prázdný\n",
    "    #     bt[i][0] = 1\n",
    "\n",
    "    # for i in range(1, m+1):\n",
    "    #     ai = query[i-1]\n",
    "    #     for j in range(1, n+1):\n",
    "    #         bj = target[j-1]\n",
    "    #         cost = 0 if ai == bj else 1\n",
    "    #         # kandidáti\n",
    "    #         del_q = dp[i-1][j] + 1     # delete (mažu ai)\n",
    "    #         ins_q = dp[i][j-1] + 1     # insert (vložím bj)\n",
    "    #         sub  = dp[i-1][j-1] + cost # match/replace\n",
    "\n",
    "    #         # min + backpointer\n",
    "    #         if sub <= del_q and sub <= ins_q:\n",
    "    #             dp[i][j] = sub\n",
    "    #             bt[i][j] = 0\n",
    "    #         elif del_q <= ins_q:\n",
    "    #             dp[i][j] = del_q\n",
    "    #             bt[i][j] = 1\n",
    "    #         else:\n",
    "    #             dp[i][j] = ins_q\n",
    "    #             bt[i][j] = 2\n",
    "\n",
    "    # # nejlepší vzdálenost = min přes koncové pozice j (substring může končit kdekoliv)\n",
    "    # best = min(dp[m][1:])  # ignorujeme j=0, to by dávalo jen mazání všeho z query\n",
    "    # if max_total_ed is not None and best > max_total_ed:\n",
    "    #     return None\n",
    "\n",
    "    # hits: List[Tuple[int, int, int, List[str]]] = []\n",
    "    # for j in range(1, n+1):\n",
    "    #     if dp[m][j] != best:\n",
    "    #         continue\n",
    "    #     # backtrack z (m, j) až na řádek i==0; sloupec v tom bodě je start\n",
    "    #     i, jj = m, j\n",
    "    #     while i > 0:\n",
    "    #         move = bt[i][jj]\n",
    "    #         if move == 0:      # diag\n",
    "    #             i -= 1\n",
    "    #             jj -= 1\n",
    "    #         elif move == 1:    # up (delete)\n",
    "    #             i -= 1\n",
    "    #         else:              # left (insert)\n",
    "    #             jj -= 1\n",
    "    #     start = jj  # pozice po návratu na i==0 je začátek substringu\n",
    "    #     end = j     # substring končí na j (exclusive v Python slice)\n",
    "    #     hits.append((best, start, end, target[start:end]))\n",
    "\n",
    "    # return hits if hits else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Iterable, Set, Optional, Any\n",
    "\n",
    "Doc = List[str]\n",
    "\n",
    "@dataclass\n",
    "class SimpleIndex:\n",
    "    postings: Dict[str, List[int]]     # token -> [doc_id]\n",
    "    doc_unique: List[Set[str]]         # interní doc_id -> unikátní tokeny\n",
    "    df: Dict[str, int]                 # token -> DF\n",
    "    N: int                             # počet dokumentů\n",
    "    ids2ext: List[Any]                 # interní doc_id -> původní ID\n",
    "    ext2ids: Dict[Any, int]            # původní ID -> interní doc_id\n",
    "\n",
    "def build_inverted_index(docs: Iterable[Doc], external_ids: Optional[Iterable[Any]] = None, stop: Optional[Set[str]] = ('■')) -> SimpleIndex:\n",
    "    \"\"\"\n",
    "    Vstup: `docs` je iterovatelný přes dokumenty, každý dokument je list tokenů (List[str]).\n",
    "    Výstup: invertovaný index + základní statistiky.\n",
    "    \"\"\"\n",
    "    stop = stop or set()\n",
    "    postings_sets: Dict[str, Set[int]] = defaultdict(set)\n",
    "    doc_unique: List[Set[str]] = []\n",
    "    ext_ids: List[Any] = []\n",
    "    ext2int: Dict[Any, int] = {}\n",
    "\n",
    "    ext_iter = iter(external_ids) if external_ids is not None else None\n",
    "\n",
    "    for internal_id, tokens in enumerate(docs):\n",
    "        u = {t for t in tokens if t not in stop}\n",
    "        doc_unique.append(u)\n",
    "        for t in u:\n",
    "            postings_sets[t].add(internal_id)\n",
    "\n",
    "        if ext_iter is not None:\n",
    "            ext_id = next(ext_iter)  # vyhodí StopIteration, pokud délky nesedí\n",
    "        else:\n",
    "            ext_id = internal_id     # fallback: původní ID = interní index\n",
    "        ext_ids.append(ext_id)\n",
    "        ext2int[ext_id] = internal_id\n",
    "\n",
    "    # finalize\n",
    "    postings = {t: sorted(ids) for t, ids in postings_sets.items()}\n",
    "    df = {t: len(ids) for t, ids in postings.items()}\n",
    "    N = len(doc_unique)\n",
    "\n",
    "    return SimpleIndex(postings=postings, doc_unique=doc_unique, df=df, N=N,\n",
    "                       ids2ext=ext_ids, ext2ids=ext2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from math import ceil\n",
    "from typing import List, Set\n",
    "\n",
    "def select_documents_for_single_token(index: SimpleIndex, term: str) -> List[int]:\n",
    "    \"\"\"Vybere dokumenty obsahující daný termín.\"\"\"\n",
    "    return index.postings.get(term, [])\n",
    "\n",
    "def select_documents_for_tokens(index: SimpleIndex, terms: List[str], benchmark: float = 0.8, stop: Optional[Set[str]] = ('■')) -> Set[int]:\n",
    "    \"\"\"\n",
    "    Vybere dokumenty, které obsahují alespoň benchmark podílu UNIKÁTNÍCH tokenů z dotazu.\n",
    "    benchmark=0.8 => musí se shodnout aspoň 80 % unikátních dotazových tokenů.\n",
    "    \"\"\"\n",
    "    qset = set(terms)\n",
    "    if not qset:\n",
    "        return set()\n",
    "\n",
    "    if stop:\n",
    "        qset -= stop\n",
    "\n",
    "    counts = defaultdict(int)  # doc_id -> kolik dotazových tokenů se našlo\n",
    "    for t in qset:\n",
    "        for doc_id in index.postings.get(t, ()):\n",
    "            counts[doc_id] += 1\n",
    "\n",
    "    required = ceil(benchmark * len(qset))  # integer práh\n",
    "    return {doc_id for doc_id, c in counts.items() if c >= required}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_by_mode(mode:str, oracc_corpus: OraccCorpus):\n",
    "    if mode == 'normalised':\n",
    "        normalised_inverted_index = build_inverted_index(oracc_corpus.normalised_corpus, oracc_corpus.texts)\n",
    "        normalised_stops = set(['■', 'ina', 'ana', 'u', 'ša'])\n",
    "        return normalised_inverted_index, normalised_stops\n",
    "    elif mode == 'normalised_POS':\n",
    "        normalised_pos_inverted_index = build_inverted_index(oracc_corpus.normalised_pos_corpus, oracc_corpus.texts)\n",
    "        normalised_pos_stops = set(['■', 'ina', 'ana', 'u', 'ša'])\n",
    "        return normalised_pos_inverted_index, normalised_pos_stops\n",
    "    elif mode == 'lemma':\n",
    "        lemma_inverted_index = build_inverted_index(oracc_corpus.lemma_corpus, oracc_corpus.texts)\n",
    "        lemma_stops = set(['■', 'ina', 'ana', 'u', 'ša', 'i-na', 'a-na'])\n",
    "        return lemma_inverted_index, lemma_stops\n",
    "    elif mode == 'lemma_POS':\n",
    "        lemma_pos_inverted_index = build_inverted_index(oracc_corpus.lemma_pos_corpus, oracc_corpus.texts)\n",
    "        lemma_pos_stops = set(['■', 'ina', 'ana', 'u', 'ša', 'i-na', 'a-na'])\n",
    "        return lemma_pos_inverted_index, lemma_pos_stops\n",
    "    elif mode == 'forms':\n",
    "        forms_inverted_index = build_inverted_index(oracc_corpus.forms_corpus, oracc_corpus.texts)\n",
    "        forms_stops = set(['■', 'ina', 'ana', 'u', 'ša', 'i-na', 'a-na'])\n",
    "        return forms_inverted_index, forms_stops\n",
    "    elif mode == 'forms_POS':\n",
    "        forms_pos_inverted_index = build_inverted_index(oracc_corpus.forms_pos_corpus, oracc_corpus.texts)\n",
    "        forms_pos_stops = set(['■', 'ina', 'ana', 'u', 'ša', 'i-na', 'a-na'])\n",
    "        return forms_pos_inverted_index, forms_pos_stops\n",
    "    elif mode == 'signs':\n",
    "        signs_inverted_index = build_inverted_index(oracc_corpus.signs_corpus, oracc_corpus.texts)\n",
    "        signs_stops = set(['■'])\n",
    "        return signs_inverted_index, signs_stops\n",
    "    elif mode == 'signs_gdl':\n",
    "        signs_gdl_inverted_index = build_inverted_index(oracc_corpus.signs_gdl_corpus, oracc_corpus.texts)\n",
    "        signs_gdl_stops = set(['■'])\n",
    "        return signs_gdl_inverted_index, signs_gdl_stops\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown mode: {mode}\")\n",
    "\n",
    "def search_for_query_in_target_dataset(mode: str, query: List[str], ORACCtarget_dataset: OraccCorpus, benchmark: float = 0.8) -> Set[int]:\n",
    "    \"\"\"\n",
    "    Prohledá dataset na základě dotazu a v nich hledá intertextualitu. 1) vrátí množinu ID dokumentů, které odpovídají dotazu, 2) aplikuje na ní edit-distance fce.\n",
    "    \"\"\"\n",
    "    target_inverted_idx, stop = load_data_by_mode(mode, ORACCtarget_dataset)\n",
    "    selected_documents = select_documents_for_tokens(target_inverted_idx, query, stop=stop, benchmark=benchmark)\n",
    "\n",
    "    hits_inner_ed = {}\n",
    "    hits_token_ed = {}\n",
    "    \n",
    "    for doc_id in selected_documents:\n",
    "        ORACC_doc_id = target_inverted_idx.ids2ext[doc_id]\n",
    "        target_data = ORACCtarget_dataset.get_data_by_id(ORACC_doc_id, mode=mode)\n",
    "        \n",
    "        hits_inner = token_edit_distance_inner(query, target_data, max_total_ed=10)\n",
    "        hits_full_tokens = token_edit_distance(query, target_data, max_total_ed=10)\n",
    "\n",
    "        if hits_inner:\n",
    "            hits_inner_ed[ORACC_doc_id] = hits_inner\n",
    "        if hits_full_tokens:\n",
    "            hits_token_ed[ORACC_doc_id] = hits_full_tokens\n",
    "\n",
    "    return hits_inner_ed, hits_token_ed\n",
    "\n",
    "\n",
    "def skip_empty_query(query: List[str], stop: Set[str], min_tokens: int=2) -> bool:\n",
    "    \"\"\"\n",
    "    Check if a query is empty or contains only stop words (or if it is long enough).\n",
    "    \"\"\"\n",
    "    query_tokens = set(query) - stop\n",
    "    return [(len(query_tokens) < min_tokens), len(query_tokens)]\n",
    "\n",
    "\n",
    "def search_for_multiple_queries(mode: str, query: List[List[str]], ORACCtarget_dataset: OraccCorpus, benchmark: float=0.8, ignore_texts: List[str] = None, min_tokens: int=2, if_min_tokens_lower_tolerance_to: int=1, remove_empty_hits: bool=False, tolerance_for_inner_ed: int=5, tolerance_for_token_ed: int=2) -> Set[int]:\n",
    "    \"\"\"\n",
    "    ADD description\n",
    "    \"\"\"\n",
    "    print('Creating the index of the target dataset.')\n",
    "    target_inverted_idx, stop = load_data_by_mode(mode, ORACCtarget_dataset)\n",
    "\n",
    "    hits_inner_ed_all = {}\n",
    "    hits_token_ed_all = {}\n",
    "\n",
    "    for subquery in tqdm(query, desc=f'Searching for intertextualities of {len(query)} queries'):\n",
    "        selected_documents = select_documents_for_tokens(target_inverted_idx, subquery, stop=stop, benchmark=benchmark)\n",
    "        # print(\"Searching for subquery:\", subquery, 'in', len(selected_documents), 'documents.')\n",
    "        hits_inner_ed = {}\n",
    "        hits_token_ed = {}\n",
    "\n",
    "        # Skipping empty queries and lowering tolerance with limit queries\n",
    "        if skip_empty_query(query=subquery, stop=stop, min_tokens=min_tokens)[0]:\n",
    "            continue\n",
    "        elif skip_empty_query(query=subquery, stop=stop, min_tokens=min_tokens)[1] == min_tokens:\n",
    "            tolerance_for_token_ed = if_min_tokens_lower_tolerance_to\n",
    "        else:\n",
    "            tolerance_for_token_ed = tolerance_for_token_ed\n",
    "\n",
    "        for doc_id in selected_documents:\n",
    "            # Skip texts to be ignored (e.g., the query text itself)\n",
    "            if ignore_texts and target_inverted_idx.ids2ext[doc_id] in ignore_texts:\n",
    "                continue\n",
    "\n",
    "            ORACC_doc_id = target_inverted_idx.ids2ext[doc_id]\n",
    "            target_data = ORACCtarget_dataset.get_data_by_id(ORACC_doc_id, mode=mode)\n",
    "\n",
    "            hits_inner = token_edit_distance_inner(subquery, target_data, max_total_ed=tolerance_for_inner_ed)\n",
    "            hits_full_tokens = token_edit_distance(subquery, target_data, max_total_ed=tolerance_for_token_ed)\n",
    "\n",
    "            if hits_inner:\n",
    "                hits_inner_ed[ORACC_doc_id] = hits_inner\n",
    "            if hits_full_tokens:\n",
    "                hits_token_ed[ORACC_doc_id] = hits_full_tokens\n",
    "\n",
    "        hits_inner_ed_all[tuple(subquery)] = hits_inner_ed\n",
    "        hits_token_ed_all[tuple(subquery)] = hits_token_ed\n",
    "\n",
    "    if remove_empty_hits:\n",
    "        hits_inner_ed_all = {k: v for k, v in hits_inner_ed_all.items() if v}\n",
    "        hits_token_ed_all = {k: v for k, v in hits_token_ed_all.items() if v}\n",
    "\n",
    "    return hits_inner_ed_all, hits_token_ed_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_query_text(query:List[str], window_size:int=5, stride:int=3) -> List[str]:\n",
    "    \"\"\"\n",
    "    Parse the input query (list of strings) into strided queries of a specified size.\n",
    "    \"\"\"\n",
    "    if window_size <= 0 or stride <= 0:\n",
    "        raise ValueError('window_size and stride must be positive integers.')\n",
    "    n = len(query)\n",
    "    if n == 0:\n",
    "        return []\n",
    "    if n <= window_size:\n",
    "        return [query[:]] # short query --> one window\n",
    "\n",
    "    # standard starts by stride\n",
    "    starts = list(range(0, n - window_size + 1, stride))\n",
    "\n",
    "    # adding the last window if not present (standard window over the limit)\n",
    "    last_start = n - window_size\n",
    "    if not starts or starts[-1] != last_start:\n",
    "        starts.append(last_start)\n",
    "\n",
    "    return [query[s:s + window_size] for s in starts]\n",
    "\n",
    "\n",
    "def get_core_project(text_id: str) -> str:\n",
    "    parts = text_id.split('/')\n",
    "    return '/'.join(parts[:len(parts)-1])\n",
    "\n",
    "\n",
    "def find_intertextualities_of_text(oracc_corpus:OraccCorpus, text_id:str, windown_size:int=5, stride:int=3, mode:str='normalised', benchmark:float=0.8, ignore_itself=True, ignore_core_project=False, tolerance_for_inner_ed=5, tolerance_for_token_ed=2, if_min_tokens_lower_tolerance_to=0):\n",
    "    \"\"\"\n",
    "    Add description\n",
    "    \"\"\"\n",
    "    queries = parse_query_text(oracc_corpus.get_data_by_id(text_id, mode=mode), window_size=windown_size, stride=stride)\n",
    "    print(f'Input text has been parsed to {len(queries)} queries.')\n",
    "\n",
    "    ignore_texts = []\n",
    "    if ignore_itself:\n",
    "        ignore_texts = [text_id]\n",
    "\n",
    "    if ignore_core_project:\n",
    "        print(f'Ignoring texts from the same core project as {text_id}.')\n",
    "        core_project = get_core_project(text_id)\n",
    "        if core_project:\n",
    "            for t_id in oracc_corpus.texts:\n",
    "                if get_core_project(t_id) == core_project:\n",
    "                    ignore_texts.append(t_id)\n",
    "\n",
    "    hits_inner_ed_all, hits_token_ed_all = search_for_multiple_queries(mode=mode, query=queries, ORACCtarget_dataset=oracc_corpus, benchmark=benchmark, ignore_texts=ignore_texts, remove_empty_hits=True, tolerance_for_inner_ed=tolerance_for_inner_ed, tolerance_for_token_ed=tolerance_for_token_ed, if_min_tokens_lower_tolerance_to=if_min_tokens_lower_tolerance_to)\n",
    "    # TODO: make hits into a class object?\n",
    "\n",
    "    return hits_inner_ed_all, hits_token_ed_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits_inner, hits_tokens = find_intertextualities_of_text(full_ORACC_corpus, 'nere/Q009326', windown_size=20, stride=5, mode='signs', benchmark=0.8, ignore_itself=True, ignore_core_project=True, tolerance_for_inner_ed=10, tolerance_for_token_ed=4, if_min_tokens_lower_tolerance_to=2)\n",
    "# TODO: involve the interchangeability of signs?\n",
    "\n",
    "print('Hits based on inner edit distance:')\n",
    "for hit in hits_inner:\n",
    "    print(hit, hits_inner[hit])\n",
    "\n",
    "print('Hits based on token edit distance:')\n",
    "for hit in hits_tokens:\n",
    "    print(hit, hits_tokens[hit])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
